{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "\n",
    "from dice_loss import DiceLoss\n",
    "import torch.optim as optim\n",
    "from brain_mri_dataset import BrainMRIDatasetBuilder,BrainMRIDataset\n",
    "\n",
    "from transforms import BrainMRITransforms\n",
    "\n",
    "from BCEDiceLoss import BceDiceLoss\n",
    "\n",
    "from calculate_iou import calculate_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch\n",
    "batch_size = 2\n",
    "\n",
    "learning_rate = 0.0003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../datasets/lgg-mri-segmentation/kaggle_3m\"\n",
    "\n",
    "builder = BrainMRIDatasetBuilder(data_dir)\n",
    "df = builder.create_df()\n",
    "train_df, val_df, test_df = builder.split_df(df)\n",
    "\n",
    "transform_ = BrainMRITransforms()\n",
    "\n",
    "train_data = BrainMRIDataset(train_df, transform = transform_ ,  mask_transform= transform_)\n",
    "val_data = BrainMRIDataset(val_df, transform = transform_ ,  mask_transform= transform_)\n",
    "test_data = BrainMRIDataset(test_df, transform = transform_ ,  mask_transform= transform_)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size = batch_size , shuffle = True)\n",
    "val_dataloader = DataLoader(val_data, batch_size = batch_size , shuffle = True)\n",
    "test_dataloader = DataLoader(test_data, batch_size = batch_size , shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from timm.models.layers import trunc_normal_\n",
    "import math\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "\n",
    "class PVMLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_state = 16, d_conv = 4, expand = 2):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "        self.mamba = Mamba(\n",
    "                d_model=input_dim//8, # Model dimension d_model\n",
    "                d_state=d_state,  # SSM state expansion factor\n",
    "                d_conv=d_conv,    # Local convolution width\n",
    "                expand=expand,    # Block expansion factor\n",
    "        )\n",
    "        self.proj = nn.Linear(input_dim, output_dim)\n",
    "        self.skip_scale= nn.Parameter(torch.ones(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if x.dtype == torch.float16:\n",
    "            x = x.type(torch.float32)\n",
    "        B, C = x.shape[:2]\n",
    "        assert C == self.input_dim\n",
    "        n_tokens = x.shape[2:].numel()\n",
    "        img_dims = x.shape[2:]\n",
    "        x_flat = x.reshape(B, C, n_tokens).transpose(-1, -2)\n",
    "        x_norm = self.norm(x_flat)\n",
    "\n",
    "        x1, x2, x3, x4, x5, x6, x7, x8 = torch.chunk(x_norm, 8, dim=2)\n",
    "        x_mamba1 = self.mamba(x1) + self.skip_scale * x1\n",
    "        x_mamba2 = self.mamba(x2) + self.skip_scale * x2\n",
    "        x_mamba3 = self.mamba(x3) + self.skip_scale * x3\n",
    "        x_mamba4 = self.mamba(x4) + self.skip_scale * x4\n",
    "        x_mamba5 = self.mamba(x5) + self.skip_scale * x5\n",
    "        x_mamba6 = self.mamba(x6) + self.skip_scale * x6\n",
    "        x_mamba7 = self.mamba(x7) + self.skip_scale * x7\n",
    "        x_mamba8 = self.mamba(x8) + self.skip_scale * x8\n",
    "        x_mamba = torch.cat([x_mamba1, x_mamba2,x_mamba3,x_mamba4,x_mamba5,x_mamba6,x_mamba7,x_mamba8], dim=2)\n",
    "\n",
    "        x_mamba = self.norm(x_mamba)\n",
    "        x_mamba = self.proj(x_mamba)\n",
    "        out = x_mamba.transpose(-1, -2).reshape(B, self.output_dim, *img_dims)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Channel_Att_Bridge(nn.Module):\n",
    "    def __init__(self, c_list, split_att='fc'):\n",
    "        super().__init__()\n",
    "        c_list_sum = sum(c_list) - c_list[-1]\n",
    "        self.split_att = split_att\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.get_all_att = nn.Conv1d(1, 1, kernel_size=3, padding=1, bias=False)\n",
    "        self.att1 = nn.Linear(c_list_sum, c_list[0]) if split_att == 'fc' else nn.Conv1d(c_list_sum, c_list[0], 1)\n",
    "        self.att2 = nn.Linear(c_list_sum, c_list[1]) if split_att == 'fc' else nn.Conv1d(c_list_sum, c_list[1], 1)\n",
    "        self.att3 = nn.Linear(c_list_sum, c_list[2]) if split_att == 'fc' else nn.Conv1d(c_list_sum, c_list[2], 1)\n",
    "        self.att4 = nn.Linear(c_list_sum, c_list[3]) if split_att == 'fc' else nn.Conv1d(c_list_sum, c_list[3], 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, t1, t2, t3, t4):\n",
    "        att = torch.cat((self.avgpool(t1), \n",
    "                         self.avgpool(t2), \n",
    "                         self.avgpool(t3), \n",
    "                         self.avgpool(t4)), dim=1)\n",
    "        att = self.get_all_att(att.squeeze(-1).transpose(-1, -2))\n",
    "        if self.split_att != 'fc':\n",
    "            att = att.transpose(-1, -2)\n",
    "        att1 = self.sigmoid(self.att1(att))\n",
    "        att2 = self.sigmoid(self.att2(att))\n",
    "        att3 = self.sigmoid(self.att3(att))\n",
    "        att4 = self.sigmoid(self.att4(att))\n",
    "        if self.split_att == 'fc':\n",
    "            att1 = att1.transpose(-1, -2).unsqueeze(-1).expand_as(t1)\n",
    "            att2 = att2.transpose(-1, -2).unsqueeze(-1).expand_as(t2)\n",
    "            att3 = att3.transpose(-1, -2).unsqueeze(-1).expand_as(t3)\n",
    "            att4 = att4.transpose(-1, -2).unsqueeze(-1).expand_as(t4)\n",
    "        else:\n",
    "            att1 = att1.unsqueeze(-1).expand_as(t1)\n",
    "            att2 = att2.unsqueeze(-1).expand_as(t2)\n",
    "            att3 = att3.unsqueeze(-1).expand_as(t3)\n",
    "            att4 = att4.unsqueeze(-1).expand_as(t4)\n",
    "            \n",
    "        return att1, att2, att3, att4\n",
    "    \n",
    "    \n",
    "class Spatial_Att_Bridge(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.shared_conv2d = nn.Sequential(nn.Conv2d(2, 1, 7, stride=1, padding=9, dilation=3),\n",
    "                                          nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, t1, t2, t3, t4):\n",
    "        t_list = [t1, t2, t3, t4]\n",
    "        att_list = []\n",
    "        for t in t_list:\n",
    "            avg_out = torch.mean(t, dim=1, keepdim=True)\n",
    "            max_out, _ = torch.max(t, dim=1, keepdim=True)\n",
    "            att = torch.cat([avg_out, max_out], dim=1)\n",
    "            att = self.shared_conv2d(att)\n",
    "            att_list.append(att)\n",
    "        return att_list[0], att_list[1], att_list[2], att_list[3]\n",
    "\n",
    "    \n",
    "class SC_Att_Bridge(nn.Module):\n",
    "    def __init__(self, c_list, split_att='fc'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.catt = Channel_Att_Bridge(c_list, split_att=split_att)\n",
    "        self.satt = Spatial_Att_Bridge()\n",
    "        \n",
    "    def forward(self, t1, t2, t3, t4):\n",
    "        r1, r2, r3, r4 = t1, t2, t3, t4\n",
    "\n",
    "        satt1, satt2, satt3, satt4 = self.satt(t1, t2, t3, t4)\n",
    "        t1, t2, t3, t4= satt1 * t1, satt2 * t2, satt3 * t3, satt4 * t4\n",
    "\n",
    "        r1_, r2_, r3_, r4_ = t1, t2, t3, t4\n",
    "        t1, t2, t3, t4 = t1 + r1, t2 + r2, t3 + r3, t4 + r4\n",
    "\n",
    "        catt1, catt2, catt3, catt4 = self.catt(t1, t2, t3, t4)\n",
    "        t1, t2, t3, t4 = catt1 * t1, catt2 * t2, catt3 * t3, catt4 * t4\n",
    "\n",
    "        return t1 + r1_, t2 + r2_, t3 + r3_, t4 + r4_\n",
    "    \n",
    "\n",
    "class UltraLight_VM_UNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=1, input_channels=3, c_list=[8,16,24,32,48,64],\n",
    "                split_att='fc', bridge=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bridge = bridge\n",
    "        \n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, c_list[0], 3, stride=1, padding=1),\n",
    "        )\n",
    "        self.encoder2 =nn.Sequential(\n",
    "            nn.Conv2d(c_list[0], c_list[1], 3, stride=1, padding=1),\n",
    "        ) \n",
    "        # self.encoder3 = nn.Sequential(\n",
    "        #     nn.Conv2d(c_list[1], c_list[2], 3, stride=1, padding=1),\n",
    "        # )\n",
    "        self.encoder3 = nn.Sequential(\n",
    "            PVMLayer(input_dim=c_list[1], output_dim=c_list[2])\n",
    "        )\n",
    "        self.encoder4 = nn.Sequential(\n",
    "            PVMLayer(input_dim=c_list[2], output_dim=c_list[3])\n",
    "        )\n",
    "        self.encoder5 = nn.Sequential(\n",
    "            PVMLayer(input_dim=c_list[3], output_dim=c_list[4])\n",
    "        )\n",
    "\n",
    "        if bridge: \n",
    "            self.scab = SC_Att_Bridge(c_list, split_att)\n",
    "            print('SC_Att_Bridge was used')\n",
    "        \n",
    "        self.decoder1 = nn.Sequential(\n",
    "            PVMLayer(input_dim=c_list[4], output_dim=c_list[3])\n",
    "        ) \n",
    "        self.decoder2 = nn.Sequential(\n",
    "            PVMLayer(input_dim=c_list[3], output_dim=c_list[2])\n",
    "        ) \n",
    "        self.decoder3 = nn.Sequential(\n",
    "            PVMLayer(input_dim=c_list[2], output_dim=c_list[1])\n",
    "        )  \n",
    "        self.decoder4 = nn.Sequential(\n",
    "            nn.Conv2d(c_list[1], c_list[0], 3, stride=1, padding=1),\n",
    "        ) \n",
    "        self.ebn1 = nn.GroupNorm(4, c_list[0])\n",
    "        self.ebn2 = nn.GroupNorm(4, c_list[1])\n",
    "        self.ebn3 = nn.GroupNorm(4, c_list[2])\n",
    "        self.ebn4 = nn.GroupNorm(4, c_list[3])\n",
    "        self.dbn1 = nn.GroupNorm(4, c_list[3])\n",
    "        self.dbn2 = nn.GroupNorm(4, c_list[2])\n",
    "        self.dbn3 = nn.GroupNorm(4, c_list[1])\n",
    "        self.dbn4 = nn.GroupNorm(4, c_list[0])\n",
    "\n",
    "        self.final = nn.Conv2d(c_list[0], num_classes, kernel_size=1)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Conv1d):\n",
    "                n = m.kernel_size[0] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = F.gelu(F.max_pool2d(self.ebn1(self.encoder1(x)),2,2))\n",
    "        t1 = out # b, c0, H/2, W/2\n",
    "\n",
    "        out = F.gelu(F.max_pool2d(self.ebn2(self.encoder2(out)),2,2))\n",
    "        t2 = out # b, c1, H/4, W/4 \n",
    "\n",
    "        out = F.gelu(F.max_pool2d(self.ebn3(self.encoder3(out)),2,2))\n",
    "        t3 = out # b, c2, H/8, W/8\n",
    "        \n",
    "        out = F.gelu(F.max_pool2d(self.ebn4(self.encoder4(out)),2,2))\n",
    "        t4 = out # b, c3, H/16, W/16\n",
    "\n",
    "        # if self.bridge: t1, t2, t3, t4, t5 = self.scab(t1, t2, t3, t4, t5)\n",
    "        if self.bridge: t1, t2, t3, t4 = self.scab(t1, t2, t3, t4)\n",
    "        \n",
    "        out = F.gelu(self.encoder5(out)) # b, c4, H/16, W/16\n",
    "        \n",
    "        out4 = F.gelu(self.dbn1(self.decoder1(out)))    # b, c4, H/16, W/16\n",
    "        out4 = torch.add(out4, t4)  # b, c4, H/16, W/16\n",
    "\n",
    "        out3 = F.gelu(F.interpolate(self.dbn2(self.decoder2(out4)), scale_factor=(2,2),mode ='bilinear',align_corners=True)) # b, c3, H/8, W/8\n",
    "        out3 = torch.add(out3, t3)  # b, c2, H/8, W/8\n",
    "\n",
    "        out2 = F.gelu(F.interpolate(self.dbn3(self.decoder3(out3)),scale_factor=(2,2),mode ='bilinear',align_corners=True)) # b, c1, H/4, W/4\n",
    "        out2 = torch.add(out2, t2) # b, c1, H/4, W/4 \n",
    "        \n",
    "        out1 = F.gelu(F.interpolate(self.dbn4(self.decoder4(out2)),scale_factor=(2,2),mode ='bilinear',align_corners=True)) # b, c0, H/2, W/2\n",
    "        out1 = torch.add(out1, t1) # b, c0, H/2, W/2\n",
    "        \n",
    "        out0 = F.interpolate(self.final(out1),scale_factor=(2,2),mode ='bilinear',align_corners=True) # b, num_class, H, W\n",
    "\n",
    "     \n",
    "        return torch.sigmoid(out0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SC_Att_Bridge was used\n"
     ]
    }
   ],
   "source": [
    "# 32, 64, 128, 256, 512\n",
    "model = nn.DataParallel(UltraLight_VM_UNet(\n",
    "    num_classes=1,\n",
    "    input_channels=3,\n",
    "    # c_list=[8,16,24,32,48,64],\n",
    "    c_list=[16, 32, 64, 128, 256],\n",
    "    split_att='fc',\n",
    "    bridge=True\n",
    ")).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 176453\n",
      "Trainable parameters: 176453\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total parameters: {total_params}')\n",
    "print(f'Trainable parameters: {trainable_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = BceDiceLoss()\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"../checkpoints/modified_ultralightmunet_v3_checkpoints/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, Train Loss: 0.5184, Val Loss: 0.3532\n",
      "Epoch: 2/100, Train Loss: 0.3588, Val Loss: 0.2863\n",
      "Epoch: 3/100, Train Loss: 0.3630, Val Loss: 0.4316\n",
      "Epoch: 4/100, Train Loss: 0.3366, Val Loss: 0.2514\n",
      "Epoch: 5/100, Train Loss: 0.3098, Val Loss: 0.2322\n",
      "Epoch: 6/100, Train Loss: 0.2871, Val Loss: 0.3039\n",
      "Epoch: 7/100, Train Loss: 0.2896, Val Loss: 0.3781\n",
      "Epoch: 8/100, Train Loss: 0.3608, Val Loss: 0.2709\n",
      "Epoch: 9/100, Train Loss: 0.2871, Val Loss: 0.2060\n",
      "Epoch: 10/100, Train Loss: 0.2930, Val Loss: 0.2163\n",
      "Epoch: 11/100, Train Loss: 0.2779, Val Loss: 0.2368\n",
      "Epoch: 12/100, Train Loss: 0.2621, Val Loss: 0.3049\n",
      "Epoch: 13/100, Train Loss: 0.2636, Val Loss: 0.2189\n",
      "Epoch: 14/100, Train Loss: 0.2533, Val Loss: 0.1844\n",
      "Epoch: 15/100, Train Loss: 0.2635, Val Loss: 0.4531\n",
      "Epoch: 16/100, Train Loss: 0.2825, Val Loss: 0.2217\n",
      "Epoch: 17/100, Train Loss: 0.2344, Val Loss: 0.1971\n",
      "Epoch: 18/100, Train Loss: 0.2335, Val Loss: 0.2024\n",
      "Epoch: 19/100, Train Loss: 0.2291, Val Loss: 0.1805\n",
      "Epoch: 20/100, Train Loss: 0.2494, Val Loss: 0.1799\n",
      "Epoch: 21/100, Train Loss: 0.2004, Val Loss: 0.1634\n",
      "Epoch: 22/100, Train Loss: 0.1989, Val Loss: 0.1757\n",
      "Epoch: 23/100, Train Loss: 0.2026, Val Loss: 0.1846\n",
      "Epoch: 24/100, Train Loss: 0.2039, Val Loss: 0.1543\n",
      "Epoch: 25/100, Train Loss: 0.1899, Val Loss: 0.1759\n",
      "Epoch: 26/100, Train Loss: 0.1981, Val Loss: 0.1558\n",
      "Epoch: 27/100, Train Loss: 0.1887, Val Loss: 0.1849\n",
      "Epoch: 28/100, Train Loss: 0.2072, Val Loss: 0.1818\n",
      "Epoch: 29/100, Train Loss: 0.1843, Val Loss: 0.1569\n",
      "Epoch: 30/100, Train Loss: 0.1750, Val Loss: 0.1605\n",
      "Epoch: 31/100, Train Loss: 0.1840, Val Loss: 0.1750\n",
      "Epoch: 32/100, Train Loss: 0.1730, Val Loss: 0.1508\n",
      "Epoch: 33/100, Train Loss: 0.1771, Val Loss: 0.1614\n",
      "Epoch: 34/100, Train Loss: 0.1708, Val Loss: 0.1464\n",
      "Epoch: 35/100, Train Loss: 0.1514, Val Loss: 0.1431\n",
      "Epoch: 36/100, Train Loss: 0.1684, Val Loss: 0.1789\n",
      "Epoch: 37/100, Train Loss: 0.1689, Val Loss: 0.1461\n",
      "Epoch: 38/100, Train Loss: 0.1626, Val Loss: 0.1676\n",
      "Epoch: 39/100, Train Loss: 0.1604, Val Loss: 0.1726\n",
      "Epoch: 40/100, Train Loss: 0.1617, Val Loss: 0.1398\n",
      "Epoch: 41/100, Train Loss: 0.1575, Val Loss: 0.1789\n",
      "Epoch: 42/100, Train Loss: 0.1584, Val Loss: 0.1338\n",
      "Epoch: 43/100, Train Loss: 0.1499, Val Loss: 0.1452\n",
      "Epoch: 44/100, Train Loss: 0.1588, Val Loss: 0.1372\n",
      "Epoch: 45/100, Train Loss: 0.1548, Val Loss: 0.1302\n",
      "Epoch: 46/100, Train Loss: 0.1485, Val Loss: 0.1440\n",
      "Epoch: 47/100, Train Loss: 0.1593, Val Loss: 0.1251\n",
      "Epoch: 48/100, Train Loss: 0.1450, Val Loss: 0.1407\n",
      "Epoch: 49/100, Train Loss: 0.1373, Val Loss: 0.1429\n",
      "Epoch: 50/100, Train Loss: 0.1530, Val Loss: 0.1379\n",
      "Epoch: 51/100, Train Loss: 0.1619, Val Loss: 0.1311\n",
      "Epoch: 52/100, Train Loss: 0.1399, Val Loss: 0.1212\n",
      "Epoch: 53/100, Train Loss: 0.1552, Val Loss: 0.1414\n",
      "Epoch: 54/100, Train Loss: 0.1528, Val Loss: 0.1581\n",
      "Epoch: 55/100, Train Loss: 0.1404, Val Loss: 0.1294\n",
      "Epoch: 56/100, Train Loss: 0.1421, Val Loss: 0.1255\n",
      "Epoch: 57/100, Train Loss: 0.1491, Val Loss: 0.1404\n",
      "Epoch: 58/100, Train Loss: 0.1303, Val Loss: 0.1318\n",
      "Epoch: 59/100, Train Loss: 0.1371, Val Loss: 0.1681\n",
      "Epoch: 60/100, Train Loss: 0.1385, Val Loss: 0.1329\n",
      "Epoch: 61/100, Train Loss: 0.1319, Val Loss: 0.1591\n",
      "Epoch: 62/100, Train Loss: 0.1295, Val Loss: 0.1246\n",
      "Epoch: 63/100, Train Loss: 0.1374, Val Loss: 0.1394\n",
      "Epoch: 64/100, Train Loss: 0.1274, Val Loss: 0.1150\n",
      "Epoch: 65/100, Train Loss: 0.1235, Val Loss: 0.1240\n",
      "Epoch: 66/100, Train Loss: 0.1321, Val Loss: 0.1253\n",
      "Epoch: 67/100, Train Loss: 0.1295, Val Loss: 0.1173\n",
      "Epoch: 68/100, Train Loss: 0.1221, Val Loss: 0.1395\n",
      "Epoch: 69/100, Train Loss: 0.1266, Val Loss: 0.1206\n",
      "Epoch: 70/100, Train Loss: 0.1276, Val Loss: 0.1540\n",
      "Epoch: 71/100, Train Loss: 0.1198, Val Loss: 0.1150\n",
      "Epoch: 72/100, Train Loss: 0.1163, Val Loss: 0.1193\n",
      "Epoch: 73/100, Train Loss: 0.1293, Val Loss: 0.1167\n",
      "Epoch: 74/100, Train Loss: 0.1169, Val Loss: 0.1061\n",
      "Epoch: 75/100, Train Loss: 0.1288, Val Loss: 0.1351\n",
      "Epoch: 76/100, Train Loss: 0.1337, Val Loss: 0.1230\n",
      "Epoch: 77/100, Train Loss: 0.1231, Val Loss: 0.1190\n",
      "Epoch: 78/100, Train Loss: 0.1108, Val Loss: 0.1077\n",
      "Epoch: 79/100, Train Loss: 0.1230, Val Loss: 0.1123\n",
      "Epoch: 80/100, Train Loss: 0.1228, Val Loss: 0.1115\n",
      "Epoch: 81/100, Train Loss: 0.1244, Val Loss: 0.1221\n",
      "Epoch: 82/100, Train Loss: 0.1193, Val Loss: 0.1209\n",
      "Epoch: 83/100, Train Loss: 0.1082, Val Loss: 0.1099\n",
      "Epoch: 84/100, Train Loss: 0.1220, Val Loss: 0.1102\n",
      "Epoch: 85/100, Train Loss: 0.1135, Val Loss: 0.1149\n",
      "Epoch: 86/100, Train Loss: 0.1116, Val Loss: 0.1271\n",
      "Epoch: 87/100, Train Loss: 0.1163, Val Loss: 0.1264\n",
      "Epoch: 88/100, Train Loss: 0.1094, Val Loss: 0.1134\n",
      "Epoch: 89/100, Train Loss: 0.1050, Val Loss: 0.1231\n",
      "Epoch: 90/100, Train Loss: 0.1193, Val Loss: 0.1365\n",
      "Epoch: 91/100, Train Loss: 0.1130, Val Loss: 0.1065\n",
      "Epoch: 92/100, Train Loss: 0.1041, Val Loss: 0.1216\n",
      "Epoch: 93/100, Train Loss: 0.1195, Val Loss: 0.1080\n",
      "Epoch: 94/100, Train Loss: 0.1027, Val Loss: 0.1121\n",
      "Epoch: 95/100, Train Loss: 0.1119, Val Loss: 0.1220\n",
      "Epoch: 96/100, Train Loss: 0.1032, Val Loss: 0.1050\n",
      "Epoch: 97/100, Train Loss: 0.1031, Val Loss: 0.1170\n",
      "Epoch: 98/100, Train Loss: 0.1131, Val Loss: 0.1125\n",
      "Epoch: 99/100, Train Loss: 0.1109, Val Loss: 0.1130\n",
      "Epoch: 100/100, Train Loss: 0.1144, Val Loss: 0.1137\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "trainIOU = []\n",
    "valIOU = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "\n",
    "    # Training mode\n",
    "    model.train()\n",
    "    total_train_iou = 0\n",
    "\n",
    "    for imgs, labels in train_dataloader:\n",
    "        imgs, labels = imgs.to(device).float(), labels.to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(imgs)\n",
    "\n",
    "        loss = criterion(pred, labels)\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss.append(total_train_loss / len(train_dataloader))\n",
    "\n",
    "    # Validation mode \n",
    "    model.eval()\n",
    "    total_val_iou = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_dataloader:\n",
    "            imgs, labels = imgs.to(device).float(), labels.to(device).float()\n",
    "            \n",
    "            pred = model(imgs)\n",
    "\n",
    "            loss = criterion(pred, labels)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    total_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_loss.append(total_val_loss)\n",
    "        \n",
    "    # Print\n",
    "    print('Epoch: {}/{}, Train Loss: {:.4f}, Val Loss: {:.4f}'.format(epoch + 1, epochs, train_loss[-1], total_val_loss))\n",
    "\n",
    "    # Save checkpoint\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'modified_ultralightmunet_v3_checkpoint_epoch_{epoch+1}.pt')\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "# Assuming your model is named 'model' and you want to save its state_dict\n",
    "model_state_dict = model.state_dict()\n",
    "\n",
    "# Specify the file path where you want to save the weights\n",
    "file_path = 'modified_ultralightmunet_v3_weights.pth'\n",
    "\n",
    "# Save the model state_dict to the specified file\n",
    "torch.save(model_state_dict, file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
