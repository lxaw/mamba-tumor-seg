{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/pankratozzi/segformer-with-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from typing import List, Iterable\n",
    "\n",
    "from IPython.display import clear_output\n",
    "gc.enable()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.ops import StochasticDepth\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from brain_mri_dataset import BrainMRIDatasetBuilder,BrainMRIDataset\n",
    "from transforms import BrainMRITransforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "seed = 37\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def seed_everything(seed=37):\n",
    "#     os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "# seed_everything()\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# print(f'Currently using \"{device.upper()}\" device.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "IMAGE_SIZE = 224\n",
    "EPOCHS = 100\n",
    "NUM_CLASSES = 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = A.Compose([\n",
    "                      A.OneOf([A.HueSaturationValue(hue_shift_limit=0.2, \n",
    "                                                    sat_shift_limit=0.2, \n",
    "                                                    val_shift_limit=0.2, \n",
    "                                                    p=0.2),      \n",
    "                      A.RandomBrightnessContrast(brightness_limit=0.2, \n",
    "                                                 contrast_limit=0.2, \n",
    "                                                 p=0.9)],p=0.2),\n",
    "                      A.ToGray(p=0.1),\n",
    "                      A.OneOf(\n",
    "                              [A.HorizontalFlip(p=0.5),\n",
    "                               A.VerticalFlip(p=0.5),\n",
    "                               A.RandomRotate90(p=0.5),\n",
    "                               A.Transpose(p=0.5),\n",
    "                              ], p=0.5),\n",
    "                      A.OneOf([\n",
    "                                A.ElasticTransform(p=0.5, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
    "                                A.GridDistortion(p=0.5),\n",
    "                                A.OpticalDistortion(distort_limit=1, shift_limit=0.5, p=1),\n",
    "                            ], p=0.8),\n",
    "                      A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE, p=1),\n",
    "                      A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0),\n",
    "                      ToTensorV2(p=1.0),\n",
    "                      ], p=1.0)\n",
    "\n",
    "valid_transforms = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "        ])\n",
    "\n",
    "# valid_transforms = A.Compose([\n",
    "#                       A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE, p=1),\n",
    "#                       A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0),\n",
    "#                       ToTensorV2(p=1.0),\n",
    "#                       ], p=1.0)\n",
    "\n",
    "# invTrans = A.Compose([A.Normalize(mean=[ 0., 0., 0. ],\n",
    "#                                   std=[ 1/0.229, 1/0.224, 1/0.225 ], max_pixel_value=1.0),\n",
    "#                       A.Normalize(mean=[ -0.485, -0.456, -0.406 ],\n",
    "#                                   std=[ 1., 1., 1. ], max_pixel_value=1.0),\n",
    "#                       ], p=1.0)\n",
    "\n",
    "# def inverse_transforms(tensor):\n",
    "#     if tensor.size(0) == 1 and len(tensor.shape) == 4:\n",
    "#         tensor.squeeze_(0)\n",
    "#     sample = {\"image\": tensor.cpu().detach().numpy().transpose(1,2,0)}\n",
    "#     image = invTrans(**sample)[\"image\"]\n",
    "\n",
    "#     return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_PATH = '../datasets/tumor_segs/kaggle_3m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = Image.open(f\"{DATASET_PATH}/TCGA_CS_4941_19960909/TCGA_CS_4941_19960909_13_mask.tif\") \n",
    "# mask = np.array(mask)[..., np.newaxis]\n",
    "# mask = np.repeat(mask, 3, axis=-1)\n",
    "\n",
    "# image = Image.open(f\"{DATASET_PATH}/TCGA_CS_4941_19960909/TCGA_CS_4941_19960909_13.tif\")\n",
    "# image = np.array(image)\n",
    "\n",
    "# # image = cv2.addWeighted(image, 0.5, mask, 0.5, 0)\n",
    "\n",
    "# plt.imshow(mask)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_paths = glob(f\"{DATASET_PATH}/*/*.tif\")\n",
    "# images_paths = sorted([path for path in all_paths if \"mask\" not in path])\n",
    "# masks_paths = sorted([path[:-9] for path in all_paths if \"mask\" in path])\n",
    "# masks_paths = [path + \"_mask.tif\" for path in masks_paths]\n",
    "# del all_paths; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.DataFrame(data={\"image\": images_paths, \"mask\": masks_paths})\n",
    "\n",
    "# x_train, x_valid = train_test_split(data, train_size=0.8, shuffle=True, random_state=seed)\n",
    "# x_test, x_valid = train_test_split(x_valid, train_size=0.5, shuffle=True, random_state=seed)\n",
    "# x_train = x_train.reset_index(drop=True)\n",
    "# x_valid = x_valid.reset_index(drop=True)\n",
    "# x_test = x_test.reset_index(drop=True)\n",
    "\n",
    "# print(f\"Train size: {x_train.shape[0]}, validation size: {x_valid.shape[0]}, test size: {x_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SegDataset(Dataset):\n",
    "#     def __init__(self, data, transforms):\n",
    "#         self.data = data\n",
    "#         self.transforms = transforms\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "    \n",
    "#     def __getitem__(self, ix):\n",
    "#         row = self.data.loc[ix].squeeze()\n",
    "#         image = Image.open(row[\"image\"])\n",
    "#         image = np.array(image)\n",
    "        \n",
    "#         mask = Image.open(row[\"mask\"])\n",
    "#         mask = np.array(mask)\n",
    "        \n",
    "#         mask = np.where(mask > 127, 255, 0)\n",
    "        \n",
    "#         sample = {\"image\": image, \"mask\": mask}\n",
    "#         sample = self.transforms(**sample)\n",
    "        \n",
    "#         image, mask = sample[\"image\"], sample[\"mask\"]\n",
    "#         mask = mask[None, ...]\n",
    "        \n",
    "#         return image, mask / 255.\n",
    "    \n",
    "#     def collate_fn(self, batch):\n",
    "#         images, masks = list(zip(*batch))\n",
    "#         images, masks = [[tensor[None].to(device) for tensor in btc] for btc in [images, masks]]\n",
    "#         images, masks = [torch.cat(tensors) for tensors in [images, masks]]\n",
    "#         return images, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegDataset(Dataset):\n",
    "    def __init__(self, data, transforms):\n",
    "        self.data = data\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "\n",
    "        image = cv2.imread(self.data.iloc[ix, 0]) / 255.0\n",
    "        mask = cv2.imread(self.data.iloc[ix, 1], cv2.IMREAD_GRAYSCALE) / 255.0\n",
    "        \n",
    "        mask = np.where(mask >= 0.5, 1., 0.)\n",
    "\n",
    "        image = valid_transforms(image)\n",
    "        mask = valid_transforms(mask)\n",
    "    \n",
    "        return image, mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = SegDataset(x_train,train_transforms)\n",
    "# valid_ds = SegDataset(x_valid,valid_transforms)\n",
    "# test_ds = SegDataset(x_test,valid_transforms)\n",
    "\n",
    "# train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=train_ds.collate_fn)\n",
    "# valid_dl = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=valid_ds.collate_fn)\n",
    "# test_dl = DataLoader(test_ds, batch_size=1, shuffle=False, collate_fn=test_ds.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../datasets/lgg-mri-segmentation/kaggle_3m\"\n",
    "\n",
    "builder = BrainMRIDatasetBuilder(data_dir)\n",
    "df = builder.create_df()\n",
    "train_df, val_df, test_df = builder.split_df(df)\n",
    "\n",
    "train_data = SegDataset(train_df, transforms= train_transforms)\n",
    "val_data = SegDataset(val_df, transforms= valid_transforms)\n",
    "test_data = SegDataset(test_df, transforms= valid_transforms)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size = BATCH_SIZE , shuffle = True)\n",
    "val_dataloader = DataLoader(val_data, batch_size = BATCH_SIZE , shuffle = False)\n",
    "test_dataloader = DataLoader(test_data, batch_size = BATCH_SIZE , shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 3143, validation size: 393, test size: 393\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train size: {train_df.shape[0]}, validation size: {val_df.shape[0]}, test size: {test_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/FrancescoSaverioZuppichini/SegFormer\n",
    "\n",
    "class LayerNorm2d(nn.LayerNorm):\n",
    "    \"\"\" swap channel dim, apply layer norm and swap back: see https://github.com/pytorch/pytorch/issues/71465 \"\"\"\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, \"b c h w -> b h w c\")\n",
    "        x = super().forward(x)\n",
    "        x = rearrange(x, \"b h w c -> b c h w\")\n",
    "        return x\n",
    "    \n",
    "class OverlapPatchMerging(nn.Sequential):\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, patch_size: int, overlap_size: int\n",
    "    ):\n",
    "        super(OverlapPatchMerging, self).__init__(\n",
    "                                        nn.Conv2d(\n",
    "                                            in_channels,\n",
    "                                            out_channels,\n",
    "                                            kernel_size=patch_size,\n",
    "                                            stride=overlap_size,\n",
    "                                            padding=patch_size // 2,\n",
    "                                            bias=False\n",
    "                                        ),\n",
    "                                        LayerNorm2d(out_channels)\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional encoding eliminated as for original paper: https://arxiv.org/pdf/2105.15203.pdf\n",
    "\n",
    "class EfficientMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, channels: int, reduction_ratio: int = 1, num_heads: int = 8):\n",
    "        super(EfficientMultiHeadAttention, self).__init__()\n",
    "        self.reducer = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                channels, channels, kernel_size=reduction_ratio, stride=reduction_ratio\n",
    "            ),\n",
    "            LayerNorm2d(channels),\n",
    "        )\n",
    "        self.att = nn.MultiheadAttention(\n",
    "            channels, num_heads=num_heads, batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, _, h, w = x.shape\n",
    "        reduced_x = self.reducer(x)\n",
    "        # attention needs tensor of shape (batch, sequence_length, channels)\n",
    "        reduced_x = rearrange(reduced_x, \"b c h w -> b (h w) c\")  # K, V \n",
    "        x = rearrange(x, \"b c h w -> b (h w) c\")  # Q\n",
    "        out = self.att(query=x, key=reduced_x, value=reduced_x)[0]\n",
    "        # reshape it back to (batch, channels, height, width)\n",
    "        out = rearrange(out, \"b (h w) c -> b c h w\", h=h, w=w)\n",
    "        return out\n",
    "    \n",
    "# transformer FFN block, here fully convolutional\n",
    "\n",
    "class MixMLP(nn.Sequential):\n",
    "    def __init__(self, channels: int, expansion: int = 4):\n",
    "        super(MixMLP, self).__init__(\n",
    "            # linear layer\n",
    "            nn.Conv2d(channels, channels, kernel_size=1),\n",
    "            # depth wise conv\n",
    "            nn.Conv2d(\n",
    "                    channels,\n",
    "                    channels * expansion,\n",
    "                    kernel_size=3,\n",
    "                    groups=channels,\n",
    "                    padding=1,\n",
    "            ),\n",
    "            nn.GELU(),\n",
    "            # linear layer\n",
    "            nn.Conv2d(channels * expansion, channels, kernel_size=1),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAdd(nn.Module):\n",
    "    \"\"\" A layer that helps to add&norm MHA and FFN outputs \"\"\"\n",
    "    def __init__(self, fn):\n",
    "        super(ResidualAdd, self).__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        out = self.fn(x, **kwargs)\n",
    "        x = x + out\n",
    "        return x\n",
    "\n",
    "class SegFormerEncoderBlock(nn.Sequential):\n",
    "    \"\"\" Encoder Block: not quite orthodox as not normalizing after MHA and FFN directly \"\"\"\n",
    "    def __init__(\n",
    "                self,\n",
    "                channels: int,\n",
    "                reduction_ratio: int = 1,\n",
    "                num_heads: int = 8,\n",
    "                mlp_expansion: int = 4,\n",
    "                drop_path_prob: float = .0\n",
    "            ):\n",
    "        super(SegFormerEncoderBlock, self).__init__(\n",
    "            ResidualAdd(\n",
    "                nn.Sequential(\n",
    "                    LayerNorm2d(channels),\n",
    "                    EfficientMultiHeadAttention(channels, reduction_ratio, num_heads),\n",
    "                )\n",
    "            ),\n",
    "            ResidualAdd(\n",
    "                nn.Sequential(\n",
    "                    LayerNorm2d(channels),\n",
    "                    MixMLP(channels, expansion=mlp_expansion),\n",
    "                    StochasticDepth(p=drop_path_prob, mode=\"batch\")  # https://pytorch.org/vision/main/generated/torchvision.ops.stochastic_depth.html\n",
    "                )\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegFormerEncoderStage(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        patch_size: int,\n",
    "        overlap_size: int,\n",
    "        drop_probs: List[int],\n",
    "        depth: int = 2,\n",
    "        reduction_ratio: int = 1,\n",
    "        num_heads: int = 8,\n",
    "        mlp_expansion: int = 4,\n",
    "    ):\n",
    "        super(SegFormerEncoderStage, self).__init__()\n",
    "        self.overlap_patch_merge = OverlapPatchMerging(\n",
    "            in_channels, out_channels, patch_size, overlap_size,\n",
    "        )\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[\n",
    "                SegFormerEncoderBlock(\n",
    "                    out_channels, reduction_ratio, num_heads, mlp_expansion, drop_probs[i]\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = LayerNorm2d(out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(data: Iterable, sizes: List[int]):\n",
    "    \"\"\"\n",
    "    Given an iterable, returns slices using sizes as indices\n",
    "    \"\"\"\n",
    "    curr = 0\n",
    "    for size in sizes:\n",
    "        chunk = data[curr: curr + size]\n",
    "        curr += size\n",
    "        yield chunk\n",
    "        \n",
    "class SegFormerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        widths: List[int],\n",
    "        depths: List[int],\n",
    "        all_num_heads: List[int],\n",
    "        patch_sizes: List[int],\n",
    "        overlap_sizes: List[int],\n",
    "        reduction_ratios: List[int],\n",
    "        mlp_expansions: List[int],\n",
    "        drop_prob: float = .0\n",
    "    ):\n",
    "        super(SegFormerEncoder, self).__init__()\n",
    "        # create drop paths probabilities (one for each stage's block)\n",
    "        drop_probs =  [x.item() for x in torch.linspace(0, drop_prob, sum(depths))]\n",
    "        self.stages = nn.ModuleList(\n",
    "            [\n",
    "                SegFormerEncoderStage(*args)\n",
    "                for args in zip(\n",
    "                    [in_channels, *widths],\n",
    "                    widths,\n",
    "                    patch_sizes,\n",
    "                    overlap_sizes,\n",
    "                    chunks(drop_probs, sizes=depths),\n",
    "                    depths,\n",
    "                    reduction_ratios,\n",
    "                    all_num_heads,\n",
    "                    mlp_expansions\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for stage in self.stages:\n",
    "            x = stage(x)\n",
    "            features.append(x)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder \n",
    "class SegFormerDecoderBlock(nn.Sequential):\n",
    "    \"\"\" upsample \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, scale_factor: int = 2):\n",
    "        super(SegFormerDecoderBlock, self).__init__(\n",
    "            nn.UpsamplingBilinear2d(scale_factor=scale_factor),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "        )\n",
    "        \n",
    "class SegFormerDecoder(nn.Module):\n",
    "    \"\"\" takes a list of features and returns a list of new features with the same spatial size and channels \"\"\"\n",
    "    def __init__(self, out_channels: int, widths: List[int], scale_factors: List[int]):\n",
    "        super(SegFormerDecoder, self).__init__()\n",
    "        self.stages = nn.ModuleList(\n",
    "            [\n",
    "                SegFormerDecoderBlock(in_channels, out_channels, scale_factor)\n",
    "                for in_channels, scale_factor in zip(widths, scale_factors)\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def forward(self, features):\n",
    "        new_features = []\n",
    "        for feature, stage in zip(features,self.stages):\n",
    "            x = stage(feature)\n",
    "            new_features.append(x)\n",
    "        return new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegFormerSegmentationHead(nn.Module):\n",
    "    def __init__(self, channels: int, num_classes: int, num_features: int = 4):\n",
    "        super(SegFormerSegmentationHead, self).__init__()\n",
    "        self.fuse = nn.Sequential(\n",
    "            nn.Conv2d(channels * num_features, channels, kernel_size=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(channels)  # paper\n",
    "        )\n",
    "        self.predict = nn.Conv2d(channels, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = torch.cat(features, dim=1)  # concatenate all features obtained before (they have same spatial dims and channels)\n",
    "        x = self.fuse(x)  # conv and normalize\n",
    "        x = self.predict(x)  # get final feature map(s)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final model: encoder -> decoder -> head\n",
    "\n",
    "class SegFormer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        widths: List[int],\n",
    "        depths: List[int],\n",
    "        all_num_heads: List[int],\n",
    "        patch_sizes: List[int],\n",
    "        overlap_sizes: List[int],\n",
    "        reduction_ratios: List[int],\n",
    "        mlp_expansions: List[int],\n",
    "        decoder_channels: int,\n",
    "        scale_factors: List[int],\n",
    "        num_classes: int,\n",
    "        drop_prob: float = 0.0,\n",
    "    ):\n",
    "\n",
    "        super(SegFormer, self).__init__()\n",
    "        self.encoder = SegFormerEncoder(\n",
    "            in_channels,\n",
    "            widths,\n",
    "            depths,\n",
    "            all_num_heads,\n",
    "            patch_sizes,\n",
    "            overlap_sizes,\n",
    "            reduction_ratios,\n",
    "            mlp_expansions,\n",
    "            drop_prob,\n",
    "        )\n",
    "        self.decoder = SegFormerDecoder(decoder_channels, widths[::-1], scale_factors)\n",
    "        self.head = SegFormerSegmentationHead(\n",
    "            decoder_channels, num_classes, num_features=len(widths)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        features = self.decoder(features[::-1])  # in the reverse order\n",
    "        segmentation = self.head(features)\n",
    "        return segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 64, 64])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check model\n",
    "segformer = SegFormer(\n",
    "    in_channels=3,\n",
    "    widths=[64, 128, 256, 512],\n",
    "    depths=[3, 4, 6, 3],\n",
    "    all_num_heads=[1, 2, 4, 8],\n",
    "    patch_sizes=[7, 3, 3, 3],\n",
    "    overlap_sizes=[4, 2, 2, 2],\n",
    "    reduction_ratios=[8, 4, 2, 1],\n",
    "    mlp_expansions=[4, 4, 4, 4],\n",
    "    decoder_channels=256,\n",
    "    scale_factors=[8, 4, 2, 1],\n",
    "    num_classes=NUM_CLASSES,\n",
    ")\n",
    "\n",
    "segmentation = segformer(torch.randn((16, 3, 256, 256)))\n",
    "segmentation.shape # torch.Size([1, 2, 56, 56])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 17,782,913\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Count the number of parameters in the model\n",
    "num_params = count_parameters(segformer)\n",
    "print(f\"Total number of parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_batch(data, model, optimizer, criterion, threshold=0.5):\n",
    "    model.train()\n",
    "    images, masks = data\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    out = model(images)  # [B, 1, 56, 56]\n",
    "    out = nn.functional.interpolate(out, size=masks.shape[-2:], mode=\"bilinear\", align_corners=False)  # [B, 1, 224, 224]\n",
    "    \n",
    "    loss = criterion(out, masks)  # [B, 1, 224, 224] of logits and [B, 1, 224, 224] of float zeros and ones\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    out = (torch.sigmoid(out) >= threshold).to(torch.float32)\n",
    "    mask_accuracy = (out == masks).float().mean()  # binary accuracy\n",
    "    \n",
    "    return loss.item(), mask_accuracy.item()\n",
    "\n",
    "# nn.CrossEntropyLoss()(torch.randn(8,2,224,224), msk.squeeze(1).long())  # [B, 2, 224, 224] and [B, 224, 224]\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_one_batch(data, model, criterion, threshold=0.5):\n",
    "    model.eval()\n",
    "    images, masks = data\n",
    "    out = model(images)\n",
    "    out = nn.functional.interpolate(out, size=masks.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "    \n",
    "    loss = criterion(out, masks)\n",
    "    \n",
    "    out = (torch.sigmoid(out) >= threshold).to(torch.float32)\n",
    "    mask_accuracy = (out == masks).float().mean() \n",
    "    \n",
    "    return loss.item(), mask_accuracy.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_plot(model):\n",
    "    model.eval()\n",
    "    idx = np.random.randint(len(x_test))\n",
    "    image = Image.open(x_test.iloc[idx, 0]).resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "    image = np.array(image)\n",
    "    sample = {\"image\": image}\n",
    "    inputs = valid_transforms(**sample)[\"image\"].unsqueeze(0).to(device)\n",
    "    \n",
    "    mask = Image.open(x_test.iloc[idx, 1]).resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "    mask = np.array(mask)[..., np.newaxis]\n",
    "    mask = np.repeat(mask, 3, axis=-1)\n",
    "    \n",
    "    # masked_image = cv2.addWeighted(image, 0.5, mask, 0.5, 0)\n",
    "    \n",
    "    out = model(inputs)\n",
    "    out = nn.functional.interpolate(out, size=(IMAGE_SIZE, IMAGE_SIZE), mode=\"bilinear\", align_corners=False)\n",
    "    out = torch.sigmoid(out)\n",
    "    out = out.detach().cpu().numpy()[0].transpose(1,2,0)\n",
    "    out = np.where(out >= 0.5, 1, 0)\n",
    "    out = np.clip(out * 255, 0, 255)\n",
    "    out = np.repeat(out, 3, axis=-1).astype(np.uint8)\n",
    "\n",
    "    # out_image = cv2.addWeighted(image, 0.5, out, 0.5, 0)\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10,5))\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title(\"Image\")\n",
    "    axes[1].imshow(mask)\n",
    "    axes[1].set_title(\"Original Mask\")\n",
    "    axes[2].imshow(out)\n",
    "    axes[2].set_title(\"Predicted Mask\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15, min_delta=0, path='model.pth'):\n",
    "        self.path = path\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model=None):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif self.best_loss - val_loss > self.min_delta:\n",
    "            checkpoint = {\n",
    "                'model': model,\n",
    "            }\n",
    "            torch.save(checkpoint, self.path)\n",
    "            print(f'Model saved to: {self.path}')\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        elif self.best_loss - val_loss < self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                print('INFO: Early stopping')\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"../checkpoints/segformer_checkpoints/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.DataParallel(SegFormer(\n",
    "                in_channels=3,\n",
    "                widths=[64, 128, 256, 512],\n",
    "                depths=[3, 4, 6, 3],\n",
    "                all_num_heads=[1, 2, 4, 8],\n",
    "                patch_sizes=[7, 3, 3, 3],\n",
    "                overlap_sizes=[4, 2, 2, 2],\n",
    "                reduction_ratios=[8, 4, 2, 1],\n",
    "                mlp_expansions=[4, 4, 4, 4],\n",
    "                decoder_channels=256,\n",
    "                scale_factors=[8, 4, 2, 1],\n",
    "                num_classes=NUM_CLASSES,\n",
    "                                        )).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # add pos_weight? tensor of size [2,]\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, betas=(0.9, 0.999), weight_decay=0.00005)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, min_lr=1e-7, factor=0.1)\n",
    "stopper = EarlyStopping(patience=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_feq = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"../checkpoints/segformer_checkpoints/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, Train Loss: 0.6532, Val Loss: 0.5781\n",
      "Epoch: 2/100, Train Loss: 0.4899, Val Loss: 0.3196\n",
      "Epoch: 3/100, Train Loss: 0.2956, Val Loss: 0.2416\n",
      "Epoch: 4/100, Train Loss: 0.1626, Val Loss: 0.0919\n",
      "Epoch: 5/100, Train Loss: 0.0940, Val Loss: 0.1092\n",
      "Epoch: 6/100, Train Loss: 0.0621, Val Loss: 0.0470\n",
      "Epoch: 7/100, Train Loss: 0.0443, Val Loss: 0.0255\n",
      "Epoch: 8/100, Train Loss: 0.0331, Val Loss: 0.0573\n",
      "Epoch: 9/100, Train Loss: 0.0264, Val Loss: 0.0230\n",
      "Epoch: 10/100, Train Loss: 0.0218, Val Loss: 0.0157\n",
      "Epoch: 11/100, Train Loss: 0.0187, Val Loss: 0.0165\n",
      "Epoch: 12/100, Train Loss: 0.0155, Val Loss: 0.0122\n",
      "Epoch: 13/100, Train Loss: 0.0128, Val Loss: 0.0148\n",
      "Epoch: 14/100, Train Loss: 0.0111, Val Loss: 0.0127\n",
      "Epoch: 15/100, Train Loss: 0.0106, Val Loss: 0.0120\n",
      "Epoch: 16/100, Train Loss: 0.0112, Val Loss: 0.0118\n",
      "Epoch: 17/100, Train Loss: 0.0090, Val Loss: 0.0084\n",
      "Epoch: 18/100, Train Loss: 0.0079, Val Loss: 0.0096\n",
      "Epoch: 19/100, Train Loss: 0.0072, Val Loss: 0.0083\n",
      "Epoch: 20/100, Train Loss: 0.0066, Val Loss: 0.0080\n",
      "Epoch: 21/100, Train Loss: 0.0062, Val Loss: 0.0081\n",
      "Epoch: 22/100, Train Loss: 0.0060, Val Loss: 0.0148\n",
      "Epoch: 23/100, Train Loss: 0.0077, Val Loss: 0.0118\n",
      "Epoch: 24/100, Train Loss: 0.0061, Val Loss: 0.0179\n",
      "Epoch: 25/100, Train Loss: 0.0059, Val Loss: 0.0076\n",
      "Epoch: 26/100, Train Loss: 0.0050, Val Loss: 0.0073\n",
      "Epoch: 27/100, Train Loss: 0.0047, Val Loss: 0.0071\n",
      "Epoch: 28/100, Train Loss: 0.0045, Val Loss: 0.0074\n",
      "Epoch: 29/100, Train Loss: 0.0044, Val Loss: 0.0067\n",
      "Epoch: 30/100, Train Loss: 0.0042, Val Loss: 0.0070\n",
      "Epoch: 31/100, Train Loss: 0.0041, Val Loss: 0.0068\n",
      "Epoch: 32/100, Train Loss: 0.0039, Val Loss: 0.0065\n",
      "Epoch: 33/100, Train Loss: 0.0064, Val Loss: 0.0091\n",
      "Epoch: 34/100, Train Loss: 0.0047, Val Loss: 0.0066\n",
      "Epoch: 35/100, Train Loss: 0.0039, Val Loss: 0.0067\n",
      "Epoch: 36/100, Train Loss: 0.0036, Val Loss: 0.0065\n",
      "Epoch: 37/100, Train Loss: 0.0034, Val Loss: 0.0067\n",
      "Epoch: 38/100, Train Loss: 0.0034, Val Loss: 0.0067\n",
      "Epoch: 39/100, Train Loss: 0.0033, Val Loss: 0.0070\n",
      "Epoch: 40/100, Train Loss: 0.0032, Val Loss: 0.0067\n",
      "Epoch: 41/100, Train Loss: 0.0032, Val Loss: 0.0069\n",
      "Epoch: 42/100, Train Loss: 0.0032, Val Loss: 0.0070\n",
      "Epoch: 43/100, Train Loss: 0.0032, Val Loss: 0.0069\n",
      "Epoch: 44/100, Train Loss: 0.0031, Val Loss: 0.0084\n",
      "Epoch: 45/100, Train Loss: 0.0033, Val Loss: 0.0079\n",
      "Epoch: 46/100, Train Loss: 0.0036, Val Loss: 0.0087\n",
      "Epoch: 47/100, Train Loss: 0.0031, Val Loss: 0.0073\n",
      "Epoch: 48/100, Train Loss: 0.0028, Val Loss: 0.0077\n",
      "Epoch: 49/100, Train Loss: 0.0027, Val Loss: 0.0077\n",
      "Epoch: 50/100, Train Loss: 0.0026, Val Loss: 0.0071\n",
      "Epoch: 51/100, Train Loss: 0.0027, Val Loss: 0.0072\n",
      "Epoch: 52/100, Train Loss: 0.0028, Val Loss: 0.0074\n",
      "Epoch: 53/100, Train Loss: 0.0028, Val Loss: 0.0080\n",
      "Epoch: 54/100, Train Loss: 0.0089, Val Loss: 0.0086\n",
      "Epoch: 55/100, Train Loss: 0.0039, Val Loss: 0.0071\n",
      "Epoch: 56/100, Train Loss: 0.0029, Val Loss: 0.0069\n",
      "Epoch: 57/100, Train Loss: 0.0026, Val Loss: 0.0072\n",
      "Epoch: 58/100, Train Loss: 0.0025, Val Loss: 0.0070\n",
      "Epoch: 59/100, Train Loss: 0.0024, Val Loss: 0.0071\n",
      "Epoch: 60/100, Train Loss: 0.0024, Val Loss: 0.0076\n",
      "Epoch: 61/100, Train Loss: 0.0023, Val Loss: 0.0073\n",
      "Epoch: 62/100, Train Loss: 0.0023, Val Loss: 0.0076\n",
      "Epoch: 63/100, Train Loss: 0.0023, Val Loss: 0.0077\n",
      "Epoch: 64/100, Train Loss: 0.0024, Val Loss: 0.0075\n",
      "Epoch: 65/100, Train Loss: 0.0024, Val Loss: 0.0079\n",
      "Epoch: 66/100, Train Loss: 0.0023, Val Loss: 0.0079\n",
      "Epoch: 67/100, Train Loss: 0.0023, Val Loss: 0.0089\n",
      "Epoch: 68/100, Train Loss: 0.0024, Val Loss: 0.0079\n",
      "Epoch: 69/100, Train Loss: 0.0027, Val Loss: 0.0076\n",
      "Epoch: 70/100, Train Loss: 0.0026, Val Loss: 0.0072\n",
      "Epoch: 71/100, Train Loss: 0.0024, Val Loss: 0.0074\n",
      "Epoch: 72/100, Train Loss: 0.0023, Val Loss: 0.0078\n",
      "Epoch: 73/100, Train Loss: 0.0022, Val Loss: 0.0076\n",
      "Epoch: 74/100, Train Loss: 0.0021, Val Loss: 0.0079\n",
      "Epoch: 75/100, Train Loss: 0.0021, Val Loss: 0.0082\n",
      "Epoch: 76/100, Train Loss: 0.0021, Val Loss: 0.0084\n",
      "Epoch: 77/100, Train Loss: 0.0022, Val Loss: 0.0080\n",
      "Epoch: 78/100, Train Loss: 0.0023, Val Loss: 0.0078\n",
      "Epoch: 79/100, Train Loss: 0.0023, Val Loss: 0.0085\n",
      "Epoch: 80/100, Train Loss: 0.0022, Val Loss: 0.0091\n",
      "Epoch: 81/100, Train Loss: 0.0021, Val Loss: 0.0081\n",
      "Epoch: 82/100, Train Loss: 0.0021, Val Loss: 0.0082\n",
      "Epoch: 83/100, Train Loss: 0.0020, Val Loss: 0.0082\n",
      "Epoch: 84/100, Train Loss: 0.0020, Val Loss: 0.0098\n",
      "Epoch: 85/100, Train Loss: 0.0020, Val Loss: 0.0083\n",
      "Epoch: 86/100, Train Loss: 0.0021, Val Loss: 0.0088\n",
      "Epoch: 87/100, Train Loss: 0.0030, Val Loss: 0.0085\n",
      "Epoch: 88/100, Train Loss: 0.0070, Val Loss: 0.0086\n",
      "Epoch: 89/100, Train Loss: 0.0029, Val Loss: 0.0078\n",
      "Epoch: 90/100, Train Loss: 0.0023, Val Loss: 0.0081\n",
      "Epoch: 91/100, Train Loss: 0.0021, Val Loss: 0.0084\n",
      "Epoch: 92/100, Train Loss: 0.0020, Val Loss: 0.0086\n",
      "Epoch: 93/100, Train Loss: 0.0019, Val Loss: 0.0089\n",
      "Epoch: 94/100, Train Loss: 0.0018, Val Loss: 0.0095\n",
      "Epoch: 95/100, Train Loss: 0.0018, Val Loss: 0.0093\n",
      "Epoch: 96/100, Train Loss: 0.0018, Val Loss: 0.0094\n",
      "Epoch: 97/100, Train Loss: 0.0018, Val Loss: 0.0095\n",
      "Epoch: 98/100, Train Loss: 0.0018, Val Loss: 0.0096\n",
      "Epoch: 99/100, Train Loss: 0.0018, Val Loss: 0.0096\n",
      "Epoch: 100/100, Train Loss: 0.0018, Val Loss: 0.0093\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "trainIOU = []\n",
    "valIOU = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "\n",
    "    # Training mode\n",
    "    model.train()\n",
    "    total_train_iou = 0\n",
    "\n",
    "    for imgs, labels in train_dataloader:\n",
    "        imgs, labels = imgs.to(device).float(), labels.to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(imgs)\n",
    "        pred = nn.functional.interpolate(pred, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)  # [B, 1, 224, 224]\n",
    "\n",
    "        loss = criterion(pred, labels)\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss.append(total_train_loss / len(train_dataloader))\n",
    "\n",
    "    # Validation mode \n",
    "    model.eval()\n",
    "    total_val_iou = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_dataloader:\n",
    "            imgs, labels = imgs.to(device).float(), labels.to(device).float()\n",
    "            \n",
    "            pred = model(imgs)\n",
    "            pred = nn.functional.interpolate(pred, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)  # [B, 1, 224, 224]\n",
    "\n",
    "\n",
    "            loss = criterion(pred, labels)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    total_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_loss.append(total_val_loss)\n",
    "        \n",
    "    # Print\n",
    "    print('Epoch: {}/{}, Train Loss: {:.4f}, Val Loss: {:.4f}'.format(epoch + 1, epochs, train_loss[-1], total_val_loss))\n",
    "\n",
    "    # Save checkpoint\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'segformer_v2_checkpoint_epoch_{epoch+1}.pt')\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "# Assuming your model is named 'model' and you want to save its state_dict\n",
    "model_state_dict = model.state_dict()\n",
    "\n",
    "# Specify the file path where you want to save the weights\n",
    "file_path = 'segformer_v2_weights.pth'\n",
    "\n",
    "# Save the model state_dict to the specified file\n",
    "torch.save(model_state_dict, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# for epoch in range(1, EPOCHS+1):\n",
    "\n",
    "#     train_loss, train_accs = [], []\n",
    "#     for step, batch in enumerate(train_dataloader):\n",
    "#         time_1 = time.time()\n",
    "        \n",
    "#         loss, accuracy = train_one_batch(batch, model, optimizer, criterion)\n",
    "\n",
    "#         train_loss.append(loss)\n",
    "#         train_accs.append(accuracy)\n",
    "        \n",
    "#     print('epoch:', epoch, \n",
    "#             '\\tstep:', step+1, '/', len(train_dl) + 1,\n",
    "#             '\\ttrain loss:', '{:.4f}'.format(loss),\n",
    "#             '\\ttrain accuracy:','{:.4f}'.format(accuracy),\n",
    "#             '\\ttime:', '{:.4f}'.format((time.time()-time_1)*print_feq), 's')\n",
    "    \n",
    "#     valid_loss, valid_accs = [], []\n",
    "#     for batch in valid_dl:\n",
    "#         loss, accuracy = validate_one_batch(batch, model, criterion)\n",
    "        \n",
    "#         valid_loss.append(loss)\n",
    "#         valid_accs.append(accuracy)\n",
    "        \n",
    "#     print('epoch:', epoch, '/', EPOCHS+1,\n",
    "#             '\\ttrain loss:', '{:.4f}'.format(np.mean(train_loss)),\n",
    "#             '\\tvalid loss:', '{:.4f}'.format(np.mean(valid_loss)),\n",
    "#             '\\ttrain accuracy', '{:.4f}'.format(np.mean(train_accs)),\n",
    "#             '\\tvalid accuracy', '{:.4f}'.format(np.mean(valid_accs)))\n",
    "    \n",
    "#     test_plot(model)\n",
    "    \n",
    "#     stopper(np.mean(valid_loss))\n",
    "#     scheduler.step(np.mean(valid_loss))\n",
    "\n",
    "#     # Save checkpoint\n",
    "#     checkpoint_path = os.path.join(checkpoint_dir, f'segformer_checkpoint_epoch_{epoch+1}.pt')\n",
    "#     torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "# # Assuming your model is named 'model' and you want to save its state_dict\n",
    "# model_state_dict = model.state_dict()\n",
    "\n",
    "# # Specify the file path where you want to save the weights\n",
    "# file_path = 'segformer_weights.pth'\n",
    "\n",
    "# # Save the model state_dict to the specified file\n",
    "# torch.save(model_state_dict, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba-segment-2sIU9SJh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
