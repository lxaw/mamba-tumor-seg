{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "\n",
    "from dice_loss import DiceLoss\n",
    "import torch.optim as optim\n",
    "from brain_mri_dataset import BrainMRIDatasetBuilder,BrainMRIDataset\n",
    "\n",
    "from transforms import BrainMRITransforms\n",
    "\n",
    "from BCEDiceLoss import BceDiceLoss\n",
    "\n",
    "from calculate_iou import calculate_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch\n",
    "batch_size = 2\n",
    "\n",
    "learning_rate = 0.0003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../datasets/lgg-mri-segmentation/kaggle_3m\"\n",
    "\n",
    "builder = BrainMRIDatasetBuilder(data_dir)\n",
    "df = builder.create_df()\n",
    "train_df, val_df, test_df = builder.split_df(df)\n",
    "\n",
    "transform_ = BrainMRITransforms()\n",
    "\n",
    "train_data = BrainMRIDataset(train_df, transform = transform_ ,  mask_transform= transform_)\n",
    "val_data = BrainMRIDataset(val_df, transform = transform_ ,  mask_transform= transform_)\n",
    "test_data = BrainMRIDataset(test_df, transform = transform_ ,  mask_transform= transform_)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size = batch_size , shuffle = True)\n",
    "val_dataloader = DataLoader(val_data, batch_size = batch_size , shuffle = True)\n",
    "test_dataloader = DataLoader(test_data, batch_size = batch_size , shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/jccrews/miniconda3/envs/umamba/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from timm.models.layers import trunc_normal_\n",
    "import math\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "\n",
    "class PVMLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_state = 16, d_conv = 4, expand = 2):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "        self.mamba = Mamba(\n",
    "                d_model=input_dim//4, # Model dimension d_model\n",
    "                d_state=d_state,  # SSM state expansion factor\n",
    "                d_conv=d_conv,    # Local convolution width\n",
    "                expand=expand,    # Block expansion factor\n",
    "        )\n",
    "        self.proj = nn.Linear(input_dim, output_dim)\n",
    "        self.skip_scale= nn.Parameter(torch.ones(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if x.dtype == torch.float16:\n",
    "            x = x.type(torch.float32)\n",
    "        B, C = x.shape[:2]\n",
    "        assert C == self.input_dim\n",
    "        n_tokens = x.shape[2:].numel()\n",
    "        img_dims = x.shape[2:]\n",
    "        x_flat = x.reshape(B, C, n_tokens).transpose(-1, -2)\n",
    "        x_norm = self.norm(x_flat)\n",
    "\n",
    "        x1, x2, x3, x4 = torch.chunk(x_norm, 4, dim=2)\n",
    "        x_mamba1 = self.mamba(x1) + self.skip_scale * x1\n",
    "        x_mamba2 = self.mamba(x2) + self.skip_scale * x2\n",
    "        x_mamba3 = self.mamba(x3) + self.skip_scale * x3\n",
    "        x_mamba4 = self.mamba(x4) + self.skip_scale * x4\n",
    "        x_mamba = torch.cat([x_mamba1, x_mamba2,x_mamba3,x_mamba4], dim=2)\n",
    "\n",
    "        x_mamba = self.norm(x_mamba)\n",
    "        x_mamba = self.proj(x_mamba)\n",
    "        out = x_mamba.transpose(-1, -2).reshape(B, self.output_dim, *img_dims)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Channel_Att_Bridge(nn.Module):\n",
    "    def __init__(self, c_list, split_att='fc'):\n",
    "        super().__init__()\n",
    "        c_list_sum = sum(c_list) - c_list[-1]\n",
    "        self.split_att = split_att\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.get_all_att = nn.Conv1d(1, 1, kernel_size=3, padding=1, bias=False)\n",
    "        self.att1 = nn.Linear(c_list_sum, c_list[0]) if split_att == 'fc' else nn.Conv1d(c_list_sum, c_list[0], 1)\n",
    "        self.att2 = nn.Linear(c_list_sum, c_list[1]) if split_att == 'fc' else nn.Conv1d(c_list_sum, c_list[1], 1)\n",
    "        self.att3 = nn.Linear(c_list_sum, c_list[2]) if split_att == 'fc' else nn.Conv1d(c_list_sum, c_list[2], 1)\n",
    "        self.att4 = nn.Linear(c_list_sum, c_list[3]) if split_att == 'fc' else nn.Conv1d(c_list_sum, c_list[3], 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, t1, t2, t3, t4):\n",
    "        att = torch.cat((self.avgpool(t1), \n",
    "                         self.avgpool(t2), \n",
    "                         self.avgpool(t3), \n",
    "                         self.avgpool(t4)), dim=1)\n",
    "        att = self.get_all_att(att.squeeze(-1).transpose(-1, -2))\n",
    "        if self.split_att != 'fc':\n",
    "            att = att.transpose(-1, -2)\n",
    "        att1 = self.sigmoid(self.att1(att))\n",
    "        att2 = self.sigmoid(self.att2(att))\n",
    "        att3 = self.sigmoid(self.att3(att))\n",
    "        att4 = self.sigmoid(self.att4(att))\n",
    "        if self.split_att == 'fc':\n",
    "            att1 = att1.transpose(-1, -2).unsqueeze(-1).expand_as(t1)\n",
    "            att2 = att2.transpose(-1, -2).unsqueeze(-1).expand_as(t2)\n",
    "            att3 = att3.transpose(-1, -2).unsqueeze(-1).expand_as(t3)\n",
    "            att4 = att4.transpose(-1, -2).unsqueeze(-1).expand_as(t4)\n",
    "        else:\n",
    "            att1 = att1.unsqueeze(-1).expand_as(t1)\n",
    "            att2 = att2.unsqueeze(-1).expand_as(t2)\n",
    "            att3 = att3.unsqueeze(-1).expand_as(t3)\n",
    "            att4 = att4.unsqueeze(-1).expand_as(t4)\n",
    "            \n",
    "        return att1, att2, att3, att4\n",
    "    \n",
    "    \n",
    "class Spatial_Att_Bridge(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.shared_conv2d = nn.Sequential(nn.Conv2d(2, 1, 7, stride=1, padding=9, dilation=3),\n",
    "                                          nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, t1, t2, t3, t4):\n",
    "        t_list = [t1, t2, t3, t4]\n",
    "        att_list = []\n",
    "        for t in t_list:\n",
    "            avg_out = torch.mean(t, dim=1, keepdim=True)\n",
    "            max_out, _ = torch.max(t, dim=1, keepdim=True)\n",
    "            att = torch.cat([avg_out, max_out], dim=1)\n",
    "            att = self.shared_conv2d(att)\n",
    "            att_list.append(att)\n",
    "        return att_list[0], att_list[1], att_list[2], att_list[3]\n",
    "\n",
    "    \n",
    "class SC_Att_Bridge(nn.Module):\n",
    "    def __init__(self, c_list, split_att='fc'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.catt = Channel_Att_Bridge(c_list, split_att=split_att)\n",
    "        self.satt = Spatial_Att_Bridge()\n",
    "        \n",
    "    def forward(self, t1, t2, t3, t4):\n",
    "        r1, r2, r3, r4 = t1, t2, t3, t4\n",
    "\n",
    "        satt1, satt2, satt3, satt4 = self.satt(t1, t2, t3, t4)\n",
    "        t1, t2, t3, t4= satt1 * t1, satt2 * t2, satt3 * t3, satt4 * t4\n",
    "\n",
    "        r1_, r2_, r3_, r4_ = t1, t2, t3, t4\n",
    "        t1, t2, t3, t4 = t1 + r1, t2 + r2, t3 + r3, t4 + r4\n",
    "\n",
    "        catt1, catt2, catt3, catt4 = self.catt(t1, t2, t3, t4)\n",
    "        t1, t2, t3, t4 = catt1 * t1, catt2 * t2, catt3 * t3, catt4 * t4\n",
    "\n",
    "        return t1 + r1_, t2 + r2_, t3 + r3_, t4 + r4_\n",
    "    \n",
    "\n",
    "class UltraLight_VM_UNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=1, input_channels=3, c_list=[8,16,24,32,48,64],\n",
    "                split_att='fc', bridge=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bridge = bridge\n",
    "        \n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, c_list[0], 3, stride=1, padding=1),\n",
    "        )\n",
    "        self.encoder2 =nn.Sequential(\n",
    "            nn.Conv2d(c_list[0], c_list[1], 3, stride=1, padding=1),\n",
    "        ) \n",
    "        # self.encoder3 = nn.Sequential(\n",
    "        #     nn.Conv2d(c_list[1], c_list[2], 3, stride=1, padding=1),\n",
    "        # )\n",
    "        self.encoder3 = nn.Sequential(\n",
    "            PVMLayer(input_dim=c_list[1], output_dim=c_list[2])\n",
    "        )\n",
    "        self.encoder4 = nn.Sequential(\n",
    "            PVMLayer(input_dim=c_list[2], output_dim=c_list[3])\n",
    "        )\n",
    "        self.encoder5 = nn.Sequential(\n",
    "            PVMLayer(input_dim=c_list[3], output_dim=c_list[4])\n",
    "        )\n",
    "\n",
    "        if bridge: \n",
    "            self.scab = SC_Att_Bridge(c_list, split_att)\n",
    "            print('SC_Att_Bridge was used')\n",
    "        \n",
    "        self.decoder1 = nn.Sequential(\n",
    "            PVMLayer(input_dim=c_list[4], output_dim=c_list[3])\n",
    "        ) \n",
    "        self.decoder2 = nn.Sequential(\n",
    "            PVMLayer(input_dim=c_list[3], output_dim=c_list[2])\n",
    "        ) \n",
    "        self.decoder3 = nn.Sequential(\n",
    "            PVMLayer(input_dim=c_list[2], output_dim=c_list[1])\n",
    "        )  \n",
    "        self.decoder4 = nn.Sequential(\n",
    "            nn.Conv2d(c_list[1], c_list[0], 3, stride=1, padding=1),\n",
    "        ) \n",
    "        self.ebn1 = nn.GroupNorm(4, c_list[0])\n",
    "        self.ebn2 = nn.GroupNorm(4, c_list[1])\n",
    "        self.ebn3 = nn.GroupNorm(4, c_list[2])\n",
    "        self.ebn4 = nn.GroupNorm(4, c_list[3])\n",
    "        self.dbn1 = nn.GroupNorm(4, c_list[3])\n",
    "        self.dbn2 = nn.GroupNorm(4, c_list[2])\n",
    "        self.dbn3 = nn.GroupNorm(4, c_list[1])\n",
    "        self.dbn4 = nn.GroupNorm(4, c_list[0])\n",
    "\n",
    "        self.final = nn.Conv2d(c_list[0], num_classes, kernel_size=1)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Conv1d):\n",
    "                n = m.kernel_size[0] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = F.gelu(F.max_pool2d(self.ebn1(self.encoder1(x)),2,2))\n",
    "        t1 = out # b, c0, H/2, W/2\n",
    "\n",
    "        out = F.gelu(F.max_pool2d(self.ebn2(self.encoder2(out)),2,2))\n",
    "        t2 = out # b, c1, H/4, W/4 \n",
    "\n",
    "        out = F.gelu(F.max_pool2d(self.ebn3(self.encoder3(out)),2,2))\n",
    "        t3 = out # b, c2, H/8, W/8\n",
    "        \n",
    "        out = F.gelu(F.max_pool2d(self.ebn4(self.encoder4(out)),2,2))\n",
    "        t4 = out # b, c3, H/16, W/16\n",
    "\n",
    "        # if self.bridge: t1, t2, t3, t4, t5 = self.scab(t1, t2, t3, t4, t5)\n",
    "        if self.bridge: t1, t2, t3, t4 = self.scab(t1, t2, t3, t4)\n",
    "        \n",
    "        out = F.gelu(self.encoder5(out)) # b, c4, H/16, W/16\n",
    "        \n",
    "        out4 = F.gelu(self.dbn1(self.decoder1(out)))    # b, c4, H/16, W/16\n",
    "        out4 = torch.add(out4, t4)  # b, c4, H/16, W/16\n",
    "\n",
    "        out3 = F.gelu(F.interpolate(self.dbn2(self.decoder2(out4)), scale_factor=(2,2),mode ='bilinear',align_corners=True)) # b, c3, H/8, W/8\n",
    "        out3 = torch.add(out3, t3)  # b, c2, H/8, W/8\n",
    "\n",
    "        out2 = F.gelu(F.interpolate(self.dbn3(self.decoder3(out3)),scale_factor=(2,2),mode ='bilinear',align_corners=True)) # b, c1, H/4, W/4\n",
    "        out2 = torch.add(out2, t2) # b, c1, H/4, W/4 \n",
    "        \n",
    "        out1 = F.gelu(F.interpolate(self.dbn4(self.decoder4(out2)),scale_factor=(2,2),mode ='bilinear',align_corners=True)) # b, c0, H/2, W/2\n",
    "        out1 = torch.add(out1, t1) # b, c0, H/2, W/2\n",
    "        \n",
    "        out0 = F.interpolate(self.final(out1),scale_factor=(2,2),mode ='bilinear',align_corners=True) # b, num_class, H, W\n",
    "\n",
    "     \n",
    "        return torch.sigmoid(out0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SC_Att_Bridge was used\n"
     ]
    }
   ],
   "source": [
    "# 32, 64, 128, 256, 512\n",
    "model = nn.DataParallel(UltraLight_VM_UNet(\n",
    "    num_classes=1,\n",
    "    input_channels=3,\n",
    "    # c_list=[8,16,24,32,48,64],\n",
    "    c_list=[16, 32, 64, 128, 256],\n",
    "    split_att='fc',\n",
    "    bridge=True\n",
    ")).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 217165\n",
      "Trainable parameters: 217165\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total parameters: {total_params}')\n",
    "print(f'Trainable parameters: {trainable_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = BceDiceLoss()\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"../checkpoints/modified_ultralightmunet_checkpoints/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/jccrews/miniconda3/envs/umamba/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, Train Loss: 0.5012, Val Loss: 0.3423\n",
      "Epoch: 2/100, Train Loss: 0.3825, Val Loss: 0.4460\n",
      "Epoch: 3/100, Train Loss: 0.3496, Val Loss: 0.4466\n",
      "Epoch: 4/100, Train Loss: 0.3266, Val Loss: 0.2635\n",
      "Epoch: 5/100, Train Loss: 0.3389, Val Loss: 0.3081\n",
      "Epoch: 6/100, Train Loss: 0.3229, Val Loss: 0.2690\n",
      "Epoch: 7/100, Train Loss: 0.2836, Val Loss: 0.2474\n",
      "Epoch: 8/100, Train Loss: 0.2805, Val Loss: 0.2382\n",
      "Epoch: 9/100, Train Loss: 0.2917, Val Loss: 0.2167\n",
      "Epoch: 10/100, Train Loss: 0.3028, Val Loss: 0.2162\n",
      "Epoch: 11/100, Train Loss: 0.3069, Val Loss: 0.2137\n",
      "Epoch: 12/100, Train Loss: 0.2583, Val Loss: 0.2133\n",
      "Epoch: 13/100, Train Loss: 0.2450, Val Loss: 0.2481\n",
      "Epoch: 14/100, Train Loss: 0.2760, Val Loss: 0.1925\n",
      "Epoch: 15/100, Train Loss: 0.2602, Val Loss: 0.1963\n",
      "Epoch: 16/100, Train Loss: 0.2149, Val Loss: 0.1926\n",
      "Epoch: 17/100, Train Loss: 0.2207, Val Loss: 0.1875\n",
      "Epoch: 18/100, Train Loss: 0.2200, Val Loss: 0.1722\n",
      "Epoch: 19/100, Train Loss: 0.2182, Val Loss: 0.2025\n",
      "Epoch: 20/100, Train Loss: 0.2012, Val Loss: 0.1584\n",
      "Epoch: 21/100, Train Loss: 0.2176, Val Loss: 0.1771\n",
      "Epoch: 22/100, Train Loss: 0.1953, Val Loss: 0.1626\n",
      "Epoch: 23/100, Train Loss: 0.2004, Val Loss: 0.3475\n",
      "Epoch: 24/100, Train Loss: 0.2203, Val Loss: 0.3645\n",
      "Epoch: 25/100, Train Loss: 0.1962, Val Loss: 0.1804\n",
      "Epoch: 26/100, Train Loss: 0.1890, Val Loss: 0.1738\n",
      "Epoch: 27/100, Train Loss: 0.1847, Val Loss: 0.1717\n",
      "Epoch: 28/100, Train Loss: 0.1780, Val Loss: 0.1514\n",
      "Epoch: 29/100, Train Loss: 0.1719, Val Loss: 0.1993\n",
      "Epoch: 30/100, Train Loss: 0.1743, Val Loss: 0.2491\n",
      "Epoch: 31/100, Train Loss: 0.1778, Val Loss: 0.1517\n",
      "Epoch: 32/100, Train Loss: 0.1658, Val Loss: 0.1585\n",
      "Epoch: 33/100, Train Loss: 0.1547, Val Loss: 0.1516\n",
      "Epoch: 34/100, Train Loss: 0.1599, Val Loss: 0.2301\n",
      "Epoch: 35/100, Train Loss: 0.1669, Val Loss: 0.1501\n",
      "Epoch: 36/100, Train Loss: 0.1530, Val Loss: 0.1423\n",
      "Epoch: 37/100, Train Loss: 0.1665, Val Loss: 0.1409\n",
      "Epoch: 38/100, Train Loss: 0.1451, Val Loss: 0.1416\n",
      "Epoch: 39/100, Train Loss: 0.1532, Val Loss: 0.1344\n",
      "Epoch: 40/100, Train Loss: 0.1590, Val Loss: 0.1555\n",
      "Epoch: 41/100, Train Loss: 0.1478, Val Loss: 0.1577\n",
      "Epoch: 42/100, Train Loss: 0.1622, Val Loss: 0.1813\n",
      "Epoch: 43/100, Train Loss: 0.1436, Val Loss: 0.1298\n",
      "Epoch: 44/100, Train Loss: 0.1424, Val Loss: 0.1248\n",
      "Epoch: 45/100, Train Loss: 0.1386, Val Loss: 0.1284\n",
      "Epoch: 46/100, Train Loss: 0.1308, Val Loss: 0.1243\n",
      "Epoch: 47/100, Train Loss: 0.1344, Val Loss: 0.1327\n",
      "Epoch: 48/100, Train Loss: 0.1419, Val Loss: 0.1226\n",
      "Epoch: 49/100, Train Loss: 0.1316, Val Loss: 0.1292\n",
      "Epoch: 50/100, Train Loss: 0.1405, Val Loss: 0.1130\n",
      "Epoch: 51/100, Train Loss: 0.1243, Val Loss: 0.1199\n",
      "Epoch: 52/100, Train Loss: 0.1296, Val Loss: 0.1589\n",
      "Epoch: 53/100, Train Loss: 0.1389, Val Loss: 0.3421\n",
      "Epoch: 54/100, Train Loss: 0.1334, Val Loss: 0.1363\n",
      "Epoch: 55/100, Train Loss: 0.1291, Val Loss: 0.1300\n",
      "Epoch: 56/100, Train Loss: 0.1307, Val Loss: 0.1148\n",
      "Epoch: 57/100, Train Loss: 0.1378, Val Loss: 0.1215\n",
      "Epoch: 58/100, Train Loss: 0.1271, Val Loss: 0.1164\n",
      "Epoch: 59/100, Train Loss: 0.1183, Val Loss: 0.1129\n",
      "Epoch: 60/100, Train Loss: 0.1187, Val Loss: 0.1231\n",
      "Epoch: 61/100, Train Loss: 0.1194, Val Loss: 0.1469\n",
      "Epoch: 62/100, Train Loss: 0.1169, Val Loss: 0.1251\n",
      "Epoch: 63/100, Train Loss: 0.1147, Val Loss: 0.1107\n",
      "Epoch: 64/100, Train Loss: 0.1170, Val Loss: 0.1143\n",
      "Epoch: 65/100, Train Loss: 0.1175, Val Loss: 0.1540\n",
      "Epoch: 66/100, Train Loss: 0.1127, Val Loss: 0.1064\n",
      "Epoch: 67/100, Train Loss: 0.1433, Val Loss: 0.1219\n",
      "Epoch: 68/100, Train Loss: 0.1120, Val Loss: 0.1132\n",
      "Epoch: 69/100, Train Loss: 0.1144, Val Loss: 0.1161\n",
      "Epoch: 70/100, Train Loss: 0.1069, Val Loss: 0.1382\n",
      "Epoch: 71/100, Train Loss: 0.1099, Val Loss: 0.1055\n",
      "Epoch: 72/100, Train Loss: 0.1118, Val Loss: 0.1129\n",
      "Epoch: 73/100, Train Loss: 0.1166, Val Loss: 0.1068\n",
      "Epoch: 74/100, Train Loss: 0.1109, Val Loss: 0.1052\n",
      "Epoch: 75/100, Train Loss: 0.0975, Val Loss: 0.1251\n",
      "Epoch: 76/100, Train Loss: 0.1074, Val Loss: 0.1064\n",
      "Epoch: 77/100, Train Loss: 0.1087, Val Loss: 0.1059\n",
      "Epoch: 78/100, Train Loss: 0.1086, Val Loss: 0.1182\n",
      "Epoch: 79/100, Train Loss: 0.0991, Val Loss: 0.0972\n",
      "Epoch: 80/100, Train Loss: 0.0981, Val Loss: 0.1178\n",
      "Epoch: 81/100, Train Loss: 0.0962, Val Loss: 0.0998\n",
      "Epoch: 82/100, Train Loss: 0.1008, Val Loss: 0.1202\n",
      "Epoch: 83/100, Train Loss: 0.1012, Val Loss: 0.1009\n",
      "Epoch: 84/100, Train Loss: 0.1083, Val Loss: 0.1001\n",
      "Epoch: 85/100, Train Loss: 0.1013, Val Loss: 0.1044\n",
      "Epoch: 86/100, Train Loss: 0.0972, Val Loss: 0.1071\n",
      "Epoch: 87/100, Train Loss: 0.0990, Val Loss: 0.0965\n",
      "Epoch: 88/100, Train Loss: 0.0998, Val Loss: 0.1546\n",
      "Epoch: 89/100, Train Loss: 0.0985, Val Loss: 0.1087\n",
      "Epoch: 90/100, Train Loss: 0.0994, Val Loss: 0.0927\n",
      "Epoch: 91/100, Train Loss: 0.1030, Val Loss: 0.1028\n",
      "Epoch: 92/100, Train Loss: 0.0926, Val Loss: 0.1001\n",
      "Epoch: 93/100, Train Loss: 0.1025, Val Loss: 0.1109\n",
      "Epoch: 94/100, Train Loss: 0.0961, Val Loss: 0.1117\n",
      "Epoch: 95/100, Train Loss: 0.0967, Val Loss: 0.0950\n",
      "Epoch: 96/100, Train Loss: 0.0999, Val Loss: 0.0992\n",
      "Epoch: 97/100, Train Loss: 0.0917, Val Loss: 0.1041\n",
      "Epoch: 98/100, Train Loss: 0.0975, Val Loss: 0.0975\n",
      "Epoch: 99/100, Train Loss: 0.0885, Val Loss: 0.0928\n",
      "Epoch: 100/100, Train Loss: 0.0869, Val Loss: 0.0886\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "trainIOU = []\n",
    "valIOU = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "\n",
    "    # Training mode\n",
    "    model.train()\n",
    "    total_train_iou = 0\n",
    "\n",
    "    for imgs, labels in train_dataloader:\n",
    "        imgs, labels = imgs.to(device).float(), labels.to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(imgs)\n",
    "\n",
    "        loss = criterion(pred, labels)\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss.append(total_train_loss / len(train_dataloader))\n",
    "\n",
    "    # Validation mode \n",
    "    model.eval()\n",
    "    total_val_iou = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_dataloader:\n",
    "            imgs, labels = imgs.to(device).float(), labels.to(device).float()\n",
    "            \n",
    "            pred = model(imgs)\n",
    "\n",
    "            loss = criterion(pred, labels)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    total_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_loss.append(total_val_loss)\n",
    "        \n",
    "    # Print\n",
    "    print('Epoch: {}/{}, Train Loss: {:.4f}, Val Loss: {:.4f}'.format(epoch + 1, epochs, train_loss[-1], total_val_loss))\n",
    "\n",
    "    # Save checkpoint\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'modified_ultralightmunet_checkpoint_epoch_{epoch+1}.pt')\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "# Assuming your model is named 'model' and you want to save its state_dict\n",
    "model_state_dict = model.state_dict()\n",
    "\n",
    "# Specify the file path where you want to save the weights\n",
    "file_path = 'modified_ultralightmunet_weights.pth'\n",
    "\n",
    "# Save the model state_dict to the specified file\n",
    "torch.save(model_state_dict, file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
