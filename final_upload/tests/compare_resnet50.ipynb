{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "\n",
    "from dice_loss import DiceLoss\n",
    "import torch.optim as optim\n",
    "from brain_mri_dataset import BrainMRIDatasetBuilder,BrainMRIDataset\n",
    "\n",
    "from transforms import BrainMRITransforms\n",
    "\n",
    "from calculate_iou import calculate_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch\n",
    "batch_size = 64\n",
    "\n",
    "learning_rate = 0.0003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../datasets/lgg-mri-segmentation/kaggle_3m\"\n",
    "\n",
    "builder = BrainMRIDatasetBuilder(data_dir)\n",
    "df = builder.create_df()\n",
    "train_df, val_df, test_df = builder.split_df(df)\n",
    "\n",
    "transform_ = BrainMRITransforms()\n",
    "\n",
    "train_data = BrainMRIDataset(train_df, transform = transform_ ,  mask_transform= transform_)\n",
    "val_data = BrainMRIDataset(val_df, transform = transform_ ,  mask_transform= transform_)\n",
    "test_data = BrainMRIDataset(test_df, transform = transform_ ,  mask_transform= transform_)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size = batch_size , shuffle = True)\n",
    "val_dataloader = DataLoader(val_data, batch_size = batch_size , shuffle = True)\n",
    "test_dataloader = DataLoader(test_data, batch_size = batch_size , shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet50\n",
    "\n",
    "class ResNet50Seg(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(ResNet50Seg, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        self.encoder = nn.Sequential(*list(resnet.children())[:-2])  # Remove the final fully connected layer and avgpool layer\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2048, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, num_classes, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Upsample(size=(256, 256), mode='bilinear', align_corners=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/jccrews/miniconda3/envs/umamba/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/share/jccrews/miniconda3/envs/umamba/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "model = ResNet50Seg()\n",
    "input_tensor = torch.randn(1, 3, 256, 256)  # Example input tensor with size 256x256\n",
    "output = model(input_tensor)\n",
    "print(output.shape)  # Check the shape of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.DataParallel(ResNet50Seg()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): ResNet50Seg(\n",
      "    (encoder): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (4): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (5): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (7): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder): Sequential(\n",
      "      (0): ConvTranspose2d(2048, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): ConvTranspose2d(128, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (7): Upsample(size=(256, 256), mode='bilinear')\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = DiceLoss()\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"../checkpoints/resnet50_checkpoints/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/jccrews/miniconda3/envs/umamba/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, Train Loss: 0.5409, Val Loss: 0.2866\n",
      "Epoch: 2/100, Train Loss: 0.2377, Val Loss: 0.2017\n",
      "Epoch: 3/100, Train Loss: 0.1841, Val Loss: 0.1949\n",
      "Epoch: 4/100, Train Loss: 0.1594, Val Loss: 0.1622\n",
      "Epoch: 5/100, Train Loss: 0.1419, Val Loss: 0.1503\n",
      "Epoch: 6/100, Train Loss: 0.1313, Val Loss: 0.1514\n",
      "Epoch: 7/100, Train Loss: 0.1361, Val Loss: 0.2813\n",
      "Epoch: 8/100, Train Loss: 0.1514, Val Loss: 0.2004\n",
      "Epoch: 9/100, Train Loss: 0.1420, Val Loss: 0.1436\n",
      "Epoch: 10/100, Train Loss: 0.1138, Val Loss: 0.1194\n",
      "Epoch: 11/100, Train Loss: 0.1056, Val Loss: 0.1284\n",
      "Epoch: 12/100, Train Loss: 0.0991, Val Loss: 0.1385\n",
      "Epoch: 13/100, Train Loss: 0.0993, Val Loss: 0.1365\n",
      "Epoch: 14/100, Train Loss: 0.0954, Val Loss: 0.1560\n",
      "Epoch: 15/100, Train Loss: 0.0910, Val Loss: 0.1344\n",
      "Epoch: 16/100, Train Loss: 0.0900, Val Loss: 0.1367\n",
      "Epoch: 17/100, Train Loss: 0.0904, Val Loss: 0.1377\n",
      "Epoch: 18/100, Train Loss: 0.0973, Val Loss: 0.1519\n",
      "Epoch: 19/100, Train Loss: 0.0893, Val Loss: 0.1243\n",
      "Epoch: 20/100, Train Loss: 0.0833, Val Loss: 0.1254\n",
      "Epoch: 21/100, Train Loss: 0.0804, Val Loss: 0.1216\n",
      "Epoch: 22/100, Train Loss: 0.0787, Val Loss: 0.1236\n",
      "Epoch: 23/100, Train Loss: 0.0861, Val Loss: 0.1235\n",
      "Epoch: 24/100, Train Loss: 0.0777, Val Loss: 0.1371\n",
      "Epoch: 25/100, Train Loss: 0.0748, Val Loss: 0.1280\n",
      "Epoch: 26/100, Train Loss: 0.1016, Val Loss: 0.1371\n",
      "Epoch: 27/100, Train Loss: 0.2136, Val Loss: 0.4545\n",
      "Epoch: 28/100, Train Loss: 0.1808, Val Loss: 0.2225\n",
      "Epoch: 29/100, Train Loss: 0.1990, Val Loss: 0.1580\n",
      "Epoch: 30/100, Train Loss: 0.1302, Val Loss: 0.1363\n",
      "Epoch: 31/100, Train Loss: 0.1100, Val Loss: 0.1257\n",
      "Epoch: 32/100, Train Loss: 0.0983, Val Loss: 0.1313\n",
      "Epoch: 33/100, Train Loss: 0.0915, Val Loss: 0.1471\n",
      "Epoch: 34/100, Train Loss: 0.0883, Val Loss: 0.1128\n",
      "Epoch: 35/100, Train Loss: 0.0837, Val Loss: 0.1137\n",
      "Epoch: 36/100, Train Loss: 0.0778, Val Loss: 0.1433\n",
      "Epoch: 37/100, Train Loss: 0.0746, Val Loss: 0.1177\n",
      "Epoch: 38/100, Train Loss: 0.0778, Val Loss: 0.1148\n",
      "Epoch: 39/100, Train Loss: 0.0959, Val Loss: 0.1257\n",
      "Epoch: 40/100, Train Loss: 0.0834, Val Loss: 0.1334\n",
      "Epoch: 41/100, Train Loss: 0.0762, Val Loss: 0.1196\n",
      "Epoch: 42/100, Train Loss: 0.0749, Val Loss: 0.1196\n",
      "Epoch: 43/100, Train Loss: 0.0726, Val Loss: 0.1201\n",
      "Epoch: 44/100, Train Loss: 0.0690, Val Loss: 0.1139\n",
      "Epoch: 45/100, Train Loss: 0.0666, Val Loss: 0.1147\n",
      "Epoch: 46/100, Train Loss: 0.0653, Val Loss: 0.1138\n",
      "Epoch: 47/100, Train Loss: 0.0659, Val Loss: 0.1207\n",
      "Epoch: 48/100, Train Loss: 0.0625, Val Loss: 0.1174\n",
      "Epoch: 49/100, Train Loss: 0.0682, Val Loss: 0.1879\n",
      "Epoch: 50/100, Train Loss: 0.0628, Val Loss: 0.1286\n",
      "Epoch: 51/100, Train Loss: 0.0734, Val Loss: 0.1240\n",
      "Epoch: 52/100, Train Loss: 0.0668, Val Loss: 0.1307\n",
      "Epoch: 53/100, Train Loss: 0.0652, Val Loss: 0.1187\n",
      "Epoch: 54/100, Train Loss: 0.0610, Val Loss: 0.1175\n",
      "Epoch: 55/100, Train Loss: 0.0598, Val Loss: 0.1075\n",
      "Epoch: 56/100, Train Loss: 0.0608, Val Loss: 0.1127\n",
      "Epoch: 57/100, Train Loss: 0.0585, Val Loss: 0.1193\n",
      "Epoch: 58/100, Train Loss: 0.0592, Val Loss: 0.1268\n",
      "Epoch: 59/100, Train Loss: 0.0689, Val Loss: 0.1489\n",
      "Epoch: 60/100, Train Loss: 0.0710, Val Loss: 0.1771\n",
      "Epoch: 61/100, Train Loss: 0.1723, Val Loss: 0.1802\n",
      "Epoch: 62/100, Train Loss: 0.1176, Val Loss: 0.1266\n",
      "Epoch: 63/100, Train Loss: 0.1013, Val Loss: 0.1649\n",
      "Epoch: 64/100, Train Loss: 0.2132, Val Loss: 0.1662\n",
      "Epoch: 65/100, Train Loss: 0.1581, Val Loss: 0.1783\n",
      "Epoch: 66/100, Train Loss: 0.1668, Val Loss: 0.1351\n",
      "Epoch: 67/100, Train Loss: 0.1362, Val Loss: 0.1504\n",
      "Epoch: 68/100, Train Loss: 0.1155, Val Loss: 0.1324\n",
      "Epoch: 69/100, Train Loss: 0.1157, Val Loss: 0.1270\n",
      "Epoch: 70/100, Train Loss: 0.1041, Val Loss: 0.1225\n",
      "Epoch: 71/100, Train Loss: 0.0917, Val Loss: 0.1262\n",
      "Epoch: 72/100, Train Loss: 0.0894, Val Loss: 0.1340\n",
      "Epoch: 73/100, Train Loss: 0.0826, Val Loss: 0.1213\n",
      "Epoch: 74/100, Train Loss: 0.0798, Val Loss: 0.1124\n",
      "Epoch: 75/100, Train Loss: 0.0796, Val Loss: 0.1095\n",
      "Epoch: 76/100, Train Loss: 0.0777, Val Loss: 0.1672\n",
      "Epoch: 77/100, Train Loss: 0.0727, Val Loss: 0.1131\n",
      "Epoch: 78/100, Train Loss: 0.0749, Val Loss: 0.1630\n",
      "Epoch: 79/100, Train Loss: 0.0684, Val Loss: 0.1134\n",
      "Epoch: 80/100, Train Loss: 0.0655, Val Loss: 0.1048\n",
      "Epoch: 81/100, Train Loss: 0.0629, Val Loss: 0.1099\n",
      "Epoch: 82/100, Train Loss: 0.0638, Val Loss: 0.1069\n",
      "Epoch: 83/100, Train Loss: 0.0603, Val Loss: 0.1117\n",
      "Epoch: 84/100, Train Loss: 0.0613, Val Loss: 0.1056\n",
      "Epoch: 85/100, Train Loss: 0.0598, Val Loss: 0.1199\n",
      "Epoch: 86/100, Train Loss: 0.0580, Val Loss: 0.1249\n",
      "Epoch: 87/100, Train Loss: 0.0583, Val Loss: 0.1084\n",
      "Epoch: 88/100, Train Loss: 0.0558, Val Loss: 0.1211\n",
      "Epoch: 89/100, Train Loss: 0.0548, Val Loss: 0.1285\n",
      "Epoch: 90/100, Train Loss: 0.0549, Val Loss: 0.1205\n",
      "Epoch: 91/100, Train Loss: 0.0577, Val Loss: 0.1027\n",
      "Epoch: 92/100, Train Loss: 0.0546, Val Loss: 0.1019\n",
      "Epoch: 93/100, Train Loss: 0.0555, Val Loss: 0.1165\n",
      "Epoch: 94/100, Train Loss: 0.0527, Val Loss: 0.0990\n",
      "Epoch: 95/100, Train Loss: 0.0507, Val Loss: 0.1142\n",
      "Epoch: 96/100, Train Loss: 0.0518, Val Loss: 0.1047\n",
      "Epoch: 97/100, Train Loss: 0.0604, Val Loss: 0.1963\n",
      "Epoch: 98/100, Train Loss: 0.0541, Val Loss: 0.1139\n",
      "Epoch: 99/100, Train Loss: 0.0539, Val Loss: 0.1117\n",
      "Epoch: 100/100, Train Loss: 0.0497, Val Loss: 0.1859\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "\n",
    "    # Training mode\n",
    "    model.train()\n",
    "    total_train_iou = 0\n",
    "\n",
    "    for imgs, labels in train_dataloader:\n",
    "        imgs, labels = imgs.to(device).float(), labels.to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(imgs)\n",
    "\n",
    "        loss = criterion(pred, labels)\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss.append(total_train_loss / len(train_dataloader))\n",
    "\n",
    "    # Validation mode \n",
    "    model.eval()\n",
    "    total_val_iou = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_dataloader:\n",
    "            imgs, labels = imgs.to(device).float(), labels.to(device).float()\n",
    "            \n",
    "            pred = model(imgs)\n",
    "\n",
    "            loss = criterion(pred, labels)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    total_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_loss.append(total_val_loss)\n",
    "        \n",
    "    # Print\n",
    "    print('Epoch: {}/{}, Train Loss: {:.4f}, Val Loss: {:.4f}'.format(epoch + 1, epochs, train_loss[-1], total_val_loss))\n",
    "\n",
    "    # Save checkpoint\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'resnet50_checkpoint_epoch_{epoch+1}.pt')\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "# Assuming your model is named 'model' and you want to save its state_dict\n",
    "model_state_dict = model.state_dict()\n",
    "\n",
    "# Specify the file path where you want to save the weights\n",
    "file_path = 'resnet50_weights.pth'\n",
    "\n",
    "# Save the model state_dict to the specified file\n",
    "torch.save(model_state_dict, file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
