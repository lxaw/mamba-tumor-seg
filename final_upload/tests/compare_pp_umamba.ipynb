{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Our model -- UMamba w/ pyrmid. pooling -- done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/jccrews/miniconda3/envs/umamba/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from mamba_model import UMambaBot\n",
    "import os\n",
    "\n",
    "from dice_loss import DiceLoss\n",
    "import torch.optim as optim\n",
    "from brain_mri_dataset import BrainMRIDatasetBuilder,BrainMRIDataset\n",
    "\n",
    "from transforms import BrainMRITransforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch\n",
    "batch_size = 64\n",
    "\n",
    "learning_rate = 0.0003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../datasets/lgg-mri-segmentation/kaggle_3m\"\n",
    "\n",
    "builder = BrainMRIDatasetBuilder(data_dir)\n",
    "df = builder.create_df()\n",
    "train_df, val_df, test_df = builder.split_df(df)\n",
    "\n",
    "transform_ = BrainMRITransforms()\n",
    "\n",
    "train_data = BrainMRIDataset(train_df, transform = transform_ ,  mask_transform= transform_)\n",
    "val_data = BrainMRIDataset(val_df, transform = transform_ ,  mask_transform= transform_)\n",
    "test_data = BrainMRIDataset(test_df, transform = transform_ ,  mask_transform= transform_)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size = batch_size , shuffle = True)\n",
    "val_dataloader = DataLoader(val_data, batch_size = batch_size , shuffle = False)\n",
    "test_dataloader = DataLoader(test_data, batch_size = batch_size , shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.DataParallel(UMambaBot(\n",
    "    input_channels=3,  # Assuming RGB images with 3 channels\n",
    "    n_stages=5,\n",
    "    features_per_stage=(32, 64, 128, 256,512),\n",
    "    conv_op=nn.Conv2d,  # Assuming 2D convolution\n",
    "    kernel_sizes=(3, 3, 3, 3, 3),  # Adjusted kernel sizes for 2D convolution\n",
    "    strides=(1, 2, 2, 2, 2),\n",
    "    num_classes=1,\n",
    "    n_conv_per_stage=(1, 1, 1, 1, 1),\n",
    "    n_conv_per_stage_decoder=(1, 1, 1, 1),\n",
    "    conv_bias=True,\n",
    "    norm_op=nn.InstanceNorm2d,  # Assuming 2D instance normalization\n",
    "    norm_op_kwargs={},\n",
    "    dropout_op=None,\n",
    "    nonlin=nn.LeakyReLU,\n",
    "    nonlin_kwargs={'inplace': True},\n",
    "    # Pyramidal Pooling\n",
    "    ppm_pool_sizes=(1,2,3,6)\n",
    ")).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Check output size\n",
    "input_tensor = torch.randn(1, 3, 256, 256).to(device)  # Example input tensor with size 256x256\n",
    "output = model(input_tensor)\n",
    "print(output.shape)  # Check the shape of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = DiceLoss()\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"../checkpoints/pp_umamba_checkpoints/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, Train Loss: 0.9312, Val Loss: 0.9115\n",
      "Epoch: 2/100, Train Loss: 0.8871, Val Loss: 0.8663\n",
      "Epoch: 3/100, Train Loss: 0.8361, Val Loss: 0.8086\n",
      "Epoch: 4/100, Train Loss: 0.7618, Val Loss: 0.7387\n",
      "Epoch: 5/100, Train Loss: 0.6778, Val Loss: 0.6273\n",
      "Epoch: 6/100, Train Loss: 0.5927, Val Loss: 0.5514\n",
      "Epoch: 7/100, Train Loss: 0.4802, Val Loss: 0.4345\n",
      "Epoch: 8/100, Train Loss: 0.3984, Val Loss: 0.3592\n",
      "Epoch: 9/100, Train Loss: 0.3328, Val Loss: 0.3082\n",
      "Epoch: 10/100, Train Loss: 0.2916, Val Loss: 0.3022\n",
      "Epoch: 11/100, Train Loss: 0.2935, Val Loss: 0.2425\n",
      "Epoch: 12/100, Train Loss: 0.2267, Val Loss: 0.2105\n",
      "Epoch: 13/100, Train Loss: 0.1981, Val Loss: 0.1887\n",
      "Epoch: 14/100, Train Loss: 0.1829, Val Loss: 0.1749\n",
      "Epoch: 15/100, Train Loss: 0.1689, Val Loss: 0.1691\n",
      "Epoch: 16/100, Train Loss: 0.1553, Val Loss: 0.1472\n",
      "Epoch: 17/100, Train Loss: 0.1552, Val Loss: 0.1574\n",
      "Epoch: 18/100, Train Loss: 0.1458, Val Loss: 0.1415\n",
      "Epoch: 19/100, Train Loss: 0.1344, Val Loss: 0.1325\n",
      "Epoch: 20/100, Train Loss: 0.1325, Val Loss: 0.1353\n",
      "Epoch: 21/100, Train Loss: 0.1386, Val Loss: 0.1256\n",
      "Epoch: 22/100, Train Loss: 0.1208, Val Loss: 0.1244\n",
      "Epoch: 23/100, Train Loss: 0.1176, Val Loss: 0.1543\n",
      "Epoch: 24/100, Train Loss: 0.1658, Val Loss: 0.1304\n",
      "Epoch: 25/100, Train Loss: 0.1274, Val Loss: 0.1221\n",
      "Epoch: 26/100, Train Loss: 0.1159, Val Loss: 0.1158\n",
      "Epoch: 27/100, Train Loss: 0.1112, Val Loss: 0.1112\n",
      "Epoch: 28/100, Train Loss: 0.1102, Val Loss: 0.1062\n",
      "Epoch: 29/100, Train Loss: 0.1063, Val Loss: 0.1404\n",
      "Epoch: 30/100, Train Loss: 0.1583, Val Loss: 0.1168\n",
      "Epoch: 31/100, Train Loss: 0.1057, Val Loss: 0.1046\n",
      "Epoch: 32/100, Train Loss: 0.0973, Val Loss: 0.1016\n",
      "Epoch: 33/100, Train Loss: 0.0943, Val Loss: 0.0993\n",
      "Epoch: 34/100, Train Loss: 0.0922, Val Loss: 0.0972\n",
      "Epoch: 35/100, Train Loss: 0.0885, Val Loss: 0.1029\n",
      "Epoch: 36/100, Train Loss: 0.0891, Val Loss: 0.0967\n",
      "Epoch: 37/100, Train Loss: 0.0869, Val Loss: 0.0952\n",
      "Epoch: 38/100, Train Loss: 0.0839, Val Loss: 0.0973\n",
      "Epoch: 39/100, Train Loss: 0.0808, Val Loss: 0.0935\n",
      "Epoch: 40/100, Train Loss: 0.0873, Val Loss: 0.0916\n",
      "Epoch: 41/100, Train Loss: 0.0803, Val Loss: 0.0909\n",
      "Epoch: 42/100, Train Loss: 0.0828, Val Loss: 0.0979\n",
      "Epoch: 43/100, Train Loss: 0.0787, Val Loss: 0.0927\n",
      "Epoch: 44/100, Train Loss: 0.0747, Val Loss: 0.0911\n",
      "Epoch: 45/100, Train Loss: 0.0735, Val Loss: 0.0914\n",
      "Epoch: 46/100, Train Loss: 0.0752, Val Loss: 0.0883\n",
      "Epoch: 47/100, Train Loss: 0.0720, Val Loss: 0.0921\n",
      "Epoch: 48/100, Train Loss: 0.0728, Val Loss: 0.0903\n",
      "Epoch: 49/100, Train Loss: 0.0768, Val Loss: 0.0890\n",
      "Epoch: 50/100, Train Loss: 0.0740, Val Loss: 0.0889\n",
      "Epoch: 51/100, Train Loss: 0.0677, Val Loss: 0.0850\n",
      "Epoch: 52/100, Train Loss: 0.0695, Val Loss: 0.0884\n",
      "Epoch: 53/100, Train Loss: 0.0674, Val Loss: 0.0868\n",
      "Epoch: 54/100, Train Loss: 0.0671, Val Loss: 0.0894\n",
      "Epoch: 55/100, Train Loss: 0.0700, Val Loss: 0.0878\n",
      "Epoch: 56/100, Train Loss: 0.0686, Val Loss: 0.0945\n",
      "Epoch: 57/100, Train Loss: 0.0895, Val Loss: 0.1217\n",
      "Epoch: 58/100, Train Loss: 0.1171, Val Loss: 0.1018\n",
      "Epoch: 59/100, Train Loss: 0.0822, Val Loss: 0.0881\n",
      "Epoch: 60/100, Train Loss: 0.0724, Val Loss: 0.0883\n",
      "Epoch: 61/100, Train Loss: 0.0687, Val Loss: 0.0868\n",
      "Epoch: 62/100, Train Loss: 0.0649, Val Loss: 0.0869\n",
      "Epoch: 63/100, Train Loss: 0.0657, Val Loss: 0.0872\n",
      "Epoch: 64/100, Train Loss: 0.0660, Val Loss: 0.0846\n",
      "Epoch: 65/100, Train Loss: 0.0649, Val Loss: 0.0881\n",
      "Epoch: 66/100, Train Loss: 0.0609, Val Loss: 0.0831\n",
      "Epoch: 67/100, Train Loss: 0.0591, Val Loss: 0.0848\n",
      "Epoch: 68/100, Train Loss: 0.0569, Val Loss: 0.0840\n",
      "Epoch: 69/100, Train Loss: 0.0553, Val Loss: 0.0828\n",
      "Epoch: 70/100, Train Loss: 0.0545, Val Loss: 0.0837\n",
      "Epoch: 71/100, Train Loss: 0.0558, Val Loss: 0.0872\n",
      "Epoch: 72/100, Train Loss: 0.0562, Val Loss: 0.0839\n",
      "Epoch: 73/100, Train Loss: 0.0526, Val Loss: 0.0840\n",
      "Epoch: 74/100, Train Loss: 0.0533, Val Loss: 0.0879\n",
      "Epoch: 75/100, Train Loss: 0.0591, Val Loss: 0.0851\n",
      "Epoch: 76/100, Train Loss: 0.0552, Val Loss: 0.0827\n",
      "Epoch: 77/100, Train Loss: 0.0515, Val Loss: 0.0852\n",
      "Epoch: 78/100, Train Loss: 0.0599, Val Loss: 0.0842\n",
      "Epoch: 79/100, Train Loss: 0.0524, Val Loss: 0.0817\n",
      "Epoch: 80/100, Train Loss: 0.0505, Val Loss: 0.0864\n",
      "Epoch: 81/100, Train Loss: 0.0500, Val Loss: 0.0830\n",
      "Epoch: 82/100, Train Loss: 0.0476, Val Loss: 0.0827\n",
      "Epoch: 83/100, Train Loss: 0.0474, Val Loss: 0.0858\n",
      "Epoch: 84/100, Train Loss: 0.0473, Val Loss: 0.0853\n",
      "Epoch: 85/100, Train Loss: 0.0452, Val Loss: 0.0862\n",
      "Epoch: 86/100, Train Loss: 0.0449, Val Loss: 0.0819\n",
      "Epoch: 87/100, Train Loss: 0.0455, Val Loss: 0.0848\n",
      "Epoch: 88/100, Train Loss: 0.0443, Val Loss: 0.0839\n",
      "Epoch: 89/100, Train Loss: 0.0465, Val Loss: 0.0859\n",
      "Epoch: 90/100, Train Loss: 0.0436, Val Loss: 0.0827\n",
      "Epoch: 91/100, Train Loss: 0.0415, Val Loss: 0.0845\n",
      "Epoch: 92/100, Train Loss: 0.0415, Val Loss: 0.0856\n",
      "Epoch: 93/100, Train Loss: 0.0445, Val Loss: 0.0832\n",
      "Epoch: 94/100, Train Loss: 0.0419, Val Loss: 0.0831\n",
      "Epoch: 95/100, Train Loss: 0.0404, Val Loss: 0.0832\n",
      "Epoch: 96/100, Train Loss: 0.0383, Val Loss: 0.0829\n",
      "Epoch: 97/100, Train Loss: 0.0392, Val Loss: 0.0843\n",
      "Epoch: 98/100, Train Loss: 0.0380, Val Loss: 0.0819\n",
      "Epoch: 99/100, Train Loss: 0.0402, Val Loss: 0.0839\n",
      "Epoch: 100/100, Train Loss: 0.0383, Val Loss: 0.0840\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "trainIOU = []\n",
    "valIOU = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "\n",
    "    # Training mode\n",
    "    model.train()\n",
    "    total_train_iou = 0\n",
    "\n",
    "    for imgs, labels in train_dataloader:\n",
    "        imgs, labels = imgs.to(device).float(), labels.to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(imgs)\n",
    "\n",
    "        loss = criterion(pred, labels)\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss.append(total_train_loss / len(train_dataloader))\n",
    "\n",
    "    # Validation mode \n",
    "    model.eval()\n",
    "    total_val_iou = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_dataloader:\n",
    "            imgs, labels = imgs.to(device).float(), labels.to(device).float()\n",
    "            \n",
    "            pred = model(imgs)\n",
    "\n",
    "            loss = criterion(pred, labels)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    total_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_loss.append(total_val_loss)\n",
    "        \n",
    "    # Print\n",
    "    print('Epoch: {}/{}, Train Loss: {:.4f}, Val Loss: {:.4f}'.format(epoch + 1, epochs, train_loss[-1], total_val_loss))\n",
    "\n",
    "    # Save checkpoint\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'pp_umamba_checkpoint_epoch_{epoch+1}.pt')\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "# Assuming your model is named 'model' and you want to save its state_dict\n",
    "model_state_dict = model.state_dict()\n",
    "\n",
    "# Specify the file path where you want to save the weights\n",
    "file_path = 'pp_umamba_weights.pth'\n",
    "\n",
    "# Save the model state_dict to the specified file\n",
    "torch.save(model_state_dict, file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "del model\n",
    "del train_data\n",
    "del train_dataloader\n",
    "del val_data\n",
    "del val_dataloader\n",
    "del test_data\n",
    "del test_dataloader\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
